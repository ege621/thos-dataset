{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c736de30-bfd3-4501-8166-514cab0bd3b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Check for both datasets normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a60b7c-0cb9-4c95-96ee-c31d9b0e2f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "X_unlabeled1 = np.load('NEW_DATASET/interhand2.6m_keypoints30fps.npy')\n",
    "X_unlabeled2 = np.load('NEW_DATASET/hsosunlabeled.npy')\n",
    "X_mediapipe = np.load('NEW_DATASET/dataset_20240709_X_3096.npy')\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Shape of X_unlabeled1: {X_unlabeled1.shape}\")\n",
    "print(f\"Shape of X_unlabeled2: {X_unlabeled2.shape}\")\n",
    "print(f\"Shape of X_mediapipe: {X_mediapipe.shape}\")\n",
    "\n",
    "# Function to normalize the dataset element-wise\n",
    "def normalize_dataset_elementwise(dataset, frame_width=512, frame_height=334):\n",
    "    \"\"\"\n",
    "    Normalize the dataset element-wise:\n",
    "    - Normalize x (index 0, 3, 6, ...) by frame width.\n",
    "    - Normalize y (index 1, 4, 7, ...) by frame height.\n",
    "    - Normalize z (index 2, 5, 8, ...) relative to the wrist and scale by frame width.\n",
    "    \"\"\"\n",
    "    normalized_dataset = np.empty_like(dataset)  # Preserve the original shape\n",
    "    for i in range(dataset.shape[0]):\n",
    "        for j in range(dataset.shape[1]):\n",
    "            frame = dataset[i, j].reshape(-1, 3)  # Reshape into (21, 3) for processing\n",
    "            wrist_z = frame[0, 2]  # Wrist z-coordinate\n",
    "            frame[:, 0] /= frame_width  # Normalize x\n",
    "            frame[:, 1] /= frame_height  # Normalize y\n",
    "            frame[:, 2] = (frame[:, 2] - wrist_z) / frame_width  # Normalize z\n",
    "            normalized_dataset[i, j] = frame.flatten()  # Flatten back to original\n",
    "    return normalized_dataset\n",
    "\n",
    "# Normalize the dataset\n",
    "normalized_output_path = \"normalized_interhand2.6m.npy\"\n",
    "X_unlabeled1_normalized = normalize_dataset_elementwise(X_unlabeled1)\n",
    "\n",
    "# Save the normalized dataset\n",
    "np.save(normalized_output_path, X_unlabeled1_normalized)\n",
    "print(f\"Normalized dataset saved to {normalized_output_path}\")\n",
    "\n",
    "# Reload the normalized dataset to calculate ranges\n",
    "normalized_dataset = np.load(normalized_output_path)\n",
    "print(f\"Normalized dataset shape: {normalized_dataset.shape}\")\n",
    "\n",
    "# Check data ranges of the normalized dataset\n",
    "def check_data_ranges(dataset, name):\n",
    "    \"\"\"\n",
    "    Print the min and max values of the dataset.\n",
    "    \"\"\"\n",
    "    min_val = np.min(dataset)\n",
    "    max_val = np.max(dataset)\n",
    "    print(f\"{name} - Min Value: {min_val}, Max Value: {max_val}\")\n",
    "\n",
    "check_data_ranges(normalized_dataset, \"X_unlabeled1_normalized\")\n",
    "\n",
    "# Scale the normalized dataset to match the range of other datasets\n",
    "def calculate_scaling_factor(dataset1, dataset2, metric=\"range\"):\n",
    "    \"\"\"\n",
    "    Calculate a scaling factor to align the scales of two datasets based on their range or standard deviation.\n",
    "    \"\"\"\n",
    "    if metric == \"range\":\n",
    "        range1 = np.max(dataset1) - np.min(dataset1)\n",
    "        range2 = np.max(dataset2) - np.min(dataset2)\n",
    "        factor = range2 / range1\n",
    "    elif metric == \"std\":\n",
    "        std1 = np.std(dataset1)\n",
    "        std2 = np.std(dataset2)\n",
    "        factor = std2 / std1\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported metric. Use 'range' or 'std'.\")\n",
    "    return factor\n",
    "\n",
    "scaling_factor = calculate_scaling_factor(normalized_dataset, X_mediapipe, metric=\"range\")\n",
    "print(f\"Scaling factor calculated: {scaling_factor}\")\n",
    "\n",
    "# Apply scaling\n",
    "scaled_output_path = \"scaled_interhand2.6m.npy\"\n",
    "X_unlabeled1_scaled = normalized_dataset * scaling_factor\n",
    "np.save(scaled_output_path, X_unlabeled1_scaled)\n",
    "\n",
    "# Check data ranges after scaling\n",
    "scaled_dataset = np.load(scaled_output_path)\n",
    "check_data_ranges(scaled_dataset, \"X_unlabeled1_scaled\")\n",
    "check_data_ranges(X_unlabeled2, \"X_unlabeled2_normalized\")\n",
    "check_data_ranges(X_mediapipe, \"X_mediapipe_normalized\")\n",
    "\n",
    "print(f\"Final scaled dataset saved to {scaled_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e793d8-20e8-4635-9b17-4a0905175ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the normalized datasets\n",
    "X_unlabeled1_normalized = np.load('NEW_DATASET/scaled_interhand2.6m.npy', mmap_mode='r')\n",
    "X_unlabeled2_normalized = np.load('NEW_DATASET/hsosunlabeled.npy', mmap_mode='r')\n",
    "X_mediapipe_normalized = np.load('NEW_DATASET/dataset_20240709_X_3096.npy', mmap_mode='r')\n",
    "\n",
    "# Function to calculate the scaling factor\n",
    "def calculate_scaling_factor(dataset1, dataset2, metric=\"range\"):\n",
    "    \"\"\"\n",
    "    Calculate a scaling factor to align the scales of two datasets based on their range or standard deviation.\n",
    "    Args:\n",
    "        dataset1: First dataset to be scaled.\n",
    "        dataset2: Reference dataset.\n",
    "        metric: \"range\" or \"std\" to calculate the factor.\n",
    "    Returns:\n",
    "        Scaling factor.\n",
    "    \"\"\"\n",
    "    if metric == \"range\":\n",
    "        range1 = np.max(dataset1) - np.min(dataset1)\n",
    "        range2 = np.max(dataset2) - np.min(dataset2)\n",
    "        factor = range2 / range1\n",
    "    elif metric == \"std\":\n",
    "        std1 = np.std(dataset1)\n",
    "        std2 = np.std(dataset2)\n",
    "        factor = std2 / std1\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported metric. Use 'range' or 'std'.\")\n",
    "    return factor\n",
    "\n",
    "# Calculate scaling factor based on range (you can switch to \"std\" if preferred)\n",
    "scaling_factor = calculate_scaling_factor(X_unlabeled1_normalized, X_mediapipe_normalized, metric=\"range\")\n",
    "print(f\"Scaling factor calculated: {scaling_factor}\")\n",
    "\n",
    "# Apply scaling to X_unlabeled1_normalized\n",
    "scaled_output_path = \"scaled_interhand2.6m.npy\"\n",
    "with open(scaled_output_path, 'wb') as f:\n",
    "    num_sequences = len(X_unlabeled1_normalized)\n",
    "    chunk_size = 100\n",
    "    for i in range(0, num_sequences, chunk_size):\n",
    "        chunk = X_unlabeled1_normalized[i:i + chunk_size]\n",
    "        scaled_chunk = chunk * scaling_factor\n",
    "        np.save(f, scaled_chunk)\n",
    "        print(f\"Processed and saved scaled chunk {i // chunk_size + 1} of {int(np.ceil(num_sequences / chunk_size))}\")\n",
    "\n",
    "# Check data ranges after scaling\n",
    "def check_data_ranges(dataset, name):\n",
    "    min_val = np.min(dataset)\n",
    "    max_val = np.max(dataset)\n",
    "    print(f\"{name} - Min Value: {min_val}, Max Value: {max_val}\")\n",
    "\n",
    "check_data_ranges(np.load(scaled_output_path, mmap_mode='r'), \"X_unlabeled1_scaled\")\n",
    "check_data_ranges(X_unlabeled2_normalized, \"X_unlabeled2_normalized\")\n",
    "check_data_ranges(X_mediapipe_normalized, \"X_mediapipe_normalized\")\n",
    "\n",
    "print(f\"Scaled dataset saved to {scaled_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60310100-10be-4134-860f-309e721a4d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Load the normalized datasets\n",
    "X_unlabeled1_scaled = np.load('NEW_DATASET/scaled_interhand2.6m.npy', mmap_mode='r')\n",
    "X_unlabeled2_normalized = np.load('NEW_DATASET/hsosunlabeled.npy', mmap_mode='r')\n",
    "X_mediapipe_normalized = np.load('NEW_DATASET/dataset_20240709_X_3096.npy', mmap_mode='r')\n",
    "\n",
    "# Print shapes of datasets\n",
    "print(f\"X_unlabeled1_scaled shape: {X_unlabeled1_scaled.shape}\")\n",
    "print(f\"X_unlabeled2_normalized shape: {X_unlabeled2_normalized.shape}\")\n",
    "print(f\"X_mediapipe_normalized shape: {X_mediapipe_normalized.shape}\")\n",
    "\n",
    "# Ensure all datasets are 3D (samples, 30, 126)\n",
    "assert X_unlabeled1_scaled.ndim == 3 and X_unlabeled1_scaled.shape[2] == 126, \"X_unlabeled1_scaled has incorrect shape\"\n",
    "assert X_unlabeled2_normalized.ndim == 3 and X_unlabeled2_normalized.shape[2] == 126, \"X_unlabeled2_normalized has incorrect shape\"\n",
    "assert X_mediapipe_normalized.ndim == 3 and X_mediapipe_normalized.shape[2] == 126, \"X_mediapipe_normalized has incorrect shape\"\n",
    "\n",
    "# Helper function to calculate descriptive statistics\n",
    "def calculate_statistics(dataset, name):\n",
    "    mean_val = np.mean(dataset)\n",
    "    std_val = np.std(dataset)\n",
    "    variance = np.var(dataset)\n",
    "    print(f\"{name} - Mean: {mean_val:.4f}, Std Dev: {std_val:.4f}, Variance: {variance:.4f}\")\n",
    "    return mean_val, std_val, variance\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "stats_unlabeled1 = calculate_statistics(X_unlabeled1_scaled, \"X_unlabeled1_scaled\")\n",
    "stats_unlabeled2 = calculate_statistics(X_unlabeled2_normalized, \"X_unlabeled2_normalized\")\n",
    "stats_mediapipe = calculate_statistics(X_mediapipe_normalized, \"X_mediapipe_normalized\")\n",
    "\n",
    "# Helper function to plot histograms\n",
    "def plot_histograms(datasets, labels, bins=100):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for dataset, label in zip(datasets, labels):\n",
    "        plt.hist(dataset.flatten(), bins=bins, alpha=0.6, label=label, density=True)\n",
    "    plt.title(\"Dataset Value Distributions\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot histograms\n",
    "print(\"\\nPlotting Histograms:\")\n",
    "plot_histograms(\n",
    "    [X_unlabeled1_scaled, X_unlabeled2_normalized, X_mediapipe_normalized],\n",
    "    [\"X_unlabeled1_scaled\", \"X_unlabeled2_normalized\", \"X_mediapipe_normalized\"]\n",
    ")\n",
    "\n",
    "# Perform Kolmogorov-Smirnov tests\n",
    "def perform_ks_test(dataset1, dataset2, label1, label2):\n",
    "    ks_stat, p_value = ks_2samp(dataset1.flatten(), dataset2.flatten())\n",
    "    print(f\"KS Test between {label1} and {label2}: KS Stat = {ks_stat:.4f}, P-value = {p_value:.4e}\")\n",
    "\n",
    "print(\"\\nKolmogorov-Smirnov Tests:\")\n",
    "perform_ks_test(X_unlabeled1_scaled, X_unlabeled2_normalized, \"X_unlabeled1_scaled\", \"X_unlabeled2_normalized\")\n",
    "perform_ks_test(X_unlabeled1_scaled, X_mediapipe_normalized, \"X_unlabeled1_scaled\", \"X_mediapipe_normalized\")\n",
    "perform_ks_test(X_unlabeled2_normalized, X_mediapipe_normalized, \"X_unlabeled2_normalized\", \"X_mediapipe_normalized\")\n",
    "\n",
    "# Visualize random samples\n",
    "def visualize_random_samples(datasets, labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize random samples from datasets.\n",
    "\n",
    "    Args:\n",
    "    - datasets: List of datasets to visualize.\n",
    "    - labels: Corresponding labels for the datasets.\n",
    "    - num_samples: Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, len(datasets), figsize=(15, num_samples * 3))\n",
    "    for i in range(num_samples):\n",
    "        for j, (dataset, label) in enumerate(zip(datasets, labels)):\n",
    "            # Select a random sequence and its first frame\n",
    "            sequence_idx = np.random.randint(dataset.shape[0])\n",
    "            frame = dataset[sequence_idx, 0].reshape(21, 3)  # Reshape the first frame (126 -> 21, 3)\n",
    "\n",
    "            # Scatter plot for landmarks\n",
    "            axes[i, j].scatter(frame[:, 0], frame[:, 1], c=frame[:, 2], cmap='viridis')\n",
    "            axes[i, j].set_title(f\"{label} - Sample {sequence_idx + 1}\")\n",
    "            axes[i, j].set_xlim(0, 1)\n",
    "            axes[i, j].set_ylim(0, 1)\n",
    "            axes[i, j].invert_yaxis()  # Match MediaPipe's visualization style\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing Random Samples:\")\n",
    "visualize_random_samples(\n",
    "    [X_unlabeled1_scaled, X_unlabeled2_normalized, X_mediapipe_normalized],\n",
    "    [\"X_unlabeled1_scaled\", \"X_unlabeled2_normalized\", \"X_mediapipe_normalized\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ce1c0-ff7b-448d-976b-7649e13ec4b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pretrain with a giant dataset and save weights only the encoders. Next frame prediction with 15 frmaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d702f-ecaf-44e8-9c82-6a983e989afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Memory growth enabled for GPU devices.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow')\n",
    "\n",
    "# Load the unlabeled datasets\n",
    "X_unlabeled1 = np.load('NEW_DATASET/scaled_interhand2.6m.npy')  # (num_samples1, 30, 126)\n",
    "X_unlabeled2 = np.load('NEW_DATASET/hsosunlabeled.npy')         # (num_samples2, 30, 126)\n",
    "\n",
    "# Concatenate the arrays\n",
    "X_unlabeled = np.concatenate((X_unlabeled1, X_unlabeled2), axis=0)\n",
    "print(\"Shape of X_unlabeled:\", X_unlabeled.shape)\n",
    "\n",
    "# Choose which hand to pretrain on (0 for left/hand1, 1 for right/hand2)\n",
    "hand_index = 0\n",
    "if hand_index == 0:\n",
    "    X_hand = X_unlabeled[:, :, :63]  # (num_samples, 30, 63)\n",
    "    hand_name = \"hand1\"\n",
    "else:\n",
    "    X_hand = X_unlabeled[:, :, 63:]  # (num_samples, 30, 63)\n",
    "    hand_name = \"hand2\"\n",
    "\n",
    "# Split into inputs and outputs for next-frame prediction:\n",
    "# First 15 frames as input, next 15 frames as output\n",
    "X_input = X_hand[:, :15, :]   # (num_samples, 15, 63)\n",
    "y_output = X_hand[:, 15:, :]  # (num_samples, 15, 63)\n",
    "\n",
    "# Prepare decoder inputs by shifting y_output (not used directly here, but kept for reference)\n",
    "decoder_inputs = np.zeros_like(y_output)\n",
    "decoder_inputs[:, 1:, :] = y_output[:, :-1, :]  # Shifted right by one time step\n",
    "\n",
    "print(\"Shape of X_input (encoder input):\", X_input.shape)       # (num_samples, 15, 63)\n",
    "print(\"Shape of decoder_inputs:\", decoder_inputs.shape)         # (num_samples, 15, 63)\n",
    "print(\"Shape of y_output (target):\", y_output.shape)            # (num_samples, 15, 63)\n",
    "\n",
    "# Transformer Encoder Definition\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1, name_prefix=\"\"):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln1\")(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha\"\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout, name=f\"{name_prefix}_dropout\")(x)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add\")([x, inputs])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln2\")(x)\n",
    "    x_ff = layers.Dense(ff_dim, activation='gelu', name=f\"{name_prefix}_dense1\")(x)\n",
    "    x_ff = layers.Dropout(dropout, name=f\"{name_prefix}_dropout_ff\")(x_ff)\n",
    "    x_ff = layers.Dense(inputs.shape[-1], name=f\"{name_prefix}_dense2\")(x_ff)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add_ff\")([x_ff, x])\n",
    "    return x\n",
    "\n",
    "def build_pretraining_model(input_shape, head_size, num_heads, ff_dim, num_encoder_blocks, dropout=0.1):\n",
    "    # input_shape here is (15, 63), meaning we operate on 15 timesteps and 63 features per hand\n",
    "    left_input = tf.keras.Input(shape=input_shape, name=\"left_hand_input\")\n",
    "    right_input = tf.keras.Input(shape=input_shape, name=\"right_hand_input\")\n",
    "\n",
    "    # Left hand encoder\n",
    "    x_left = left_input\n",
    "    for i in range(num_encoder_blocks):\n",
    "        x_left = transformer_encoder(\n",
    "            x_left, head_size, num_heads, ff_dim, dropout, name_prefix=f\"left_enc_{i}\"\n",
    "        )\n",
    "    left_encoder_output = x_left  # (batch_size, 15, 63)\n",
    "\n",
    "    # Right hand encoder\n",
    "    x_right = right_input\n",
    "    for i in range(num_encoder_blocks):\n",
    "        x_right = transformer_encoder(\n",
    "            x_right, head_size, num_heads, ff_dim, dropout, name_prefix=f\"right_enc_{i}\"\n",
    "        )\n",
    "    right_encoder_output = x_right  # (batch_size, 15, 63)\n",
    "\n",
    "    # Concatenate outputs: (batch_size, 15, 63) + (batch_size, 15, 63) = (batch_size, 15, 126)\n",
    "    concatenated_output = layers.Concatenate(name=\"concat_encoders\")([left_encoder_output, right_encoder_output])\n",
    "\n",
    "    # Output layer predicts the next frames for both hands (126 features total) over 15 timesteps\n",
    "    output = layers.TimeDistributed(\n",
    "        layers.Dense(126, activation='linear'), name=\"output\"\n",
    "    )(concatenated_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[left_input, right_input], outputs=output, name=\"pretraining_model\")\n",
    "    return model\n",
    "\n",
    "# Define Hyperparameters\n",
    "pretrain_hyperparameters = {\n",
    "    'head_size': 64,\n",
    "    'num_heads': 4,\n",
    "    'ff_dim': 64,\n",
    "    'num_encoder_blocks': 4,\n",
    "    'dropout': 0.1,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 4096,\n",
    "    'epochs': 200\n",
    "}\n",
    "\n",
    "# Prepare Model\n",
    "# input_shape is (15,63) as per the data slicing done above\n",
    "input_shape = (15, 63)\n",
    "pretraining_model = build_pretraining_model(\n",
    "    input_shape=input_shape,\n",
    "    head_size=pretrain_hyperparameters['head_size'],\n",
    "    num_heads=pretrain_hyperparameters['num_heads'],\n",
    "    ff_dim=pretrain_hyperparameters['ff_dim'],\n",
    "    num_encoder_blocks=pretrain_hyperparameters['num_encoder_blocks'],\n",
    "    dropout=pretrain_hyperparameters['dropout']\n",
    ")\n",
    "\n",
    "# Freeze non-encoder layers if you ONLY want to train the encoder blocks\n",
    "# In this model, only the output layer is non-encoder. Let's freeze it if desired:\n",
    "for layer in pretraining_model.layers:\n",
    "    # If you consider the output layer as non-encoder, freeze it:\n",
    "    if layer.name.startswith(\"output\"):\n",
    "        layer.trainable = False\n",
    "    # All layers with \"enc_\" prefix remain trainable (these are the encoder blocks)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0)\n",
    "pretraining_model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    warmup_epochs = 20\n",
    "    total_epochs = pretrain_hyperparameters['epochs']\n",
    "    initial_lr = pretrain_hyperparameters['learning_rate']\n",
    "    min_lr = 1e-8\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warm-up\n",
    "        lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        # Linear decay\n",
    "        decay_epochs = total_epochs - warmup_epochs\n",
    "        lr = initial_lr - (initial_lr - min_lr) * (epoch - warmup_epochs + 1) / decay_epochs\n",
    "        lr = max(lr, min_lr)\n",
    "    print(f\"Epoch {epoch+1}: Learning rate is {lr:.8f}\")\n",
    "    return lr\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "callbacks = [LearningRateScheduler(lr_schedule, verbose=1)]\n",
    "\n",
    "# Match target dimension with output dimension (126 features)\n",
    "# We currently have y_output with shape (num_samples, 15, 63) for one hand.\n",
    "# The model output is (num_samples, 15, 126) for both hands. Let's replicate:\n",
    "y_output_full = np.concatenate([y_output, y_output], axis=-1)  # (num_samples, 15, 126)\n",
    "\n",
    "# Since we have only one hand data in X_input, let's feed the same X_input to both left and right.\n",
    "pretraining_model.fit(\n",
    "    [X_input, X_input],  # Two inputs\n",
    "    y_output_full,\n",
    "    epochs=pretrain_hyperparameters['epochs'],\n",
    "    batch_size=pretrain_hyperparameters['batch_size'],\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "pretraining_model.save_weights('encoder_pretrained_weights.h5')\n",
    "print(\"Pretrained encoder weights saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5a9ac-a397-4e45-bc0a-1c9c40bf22e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train the BiLSTM network with existing weights and labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e884be-e32a-4315-b95a-5a6e0a31df1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth enabled for GPU devices.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "\n",
    "# Data augmentation functions\n",
    "def add_noise(data, noise_factor=0.0185):\n",
    "    noise = np.random.randn(*data.shape) * noise_factor\n",
    "    return np.clip(data + noise, 0, 1)\n",
    "\n",
    "def scale(data, scale_factor=0.1605):\n",
    "    scale = 1 + np.random.uniform(-scale_factor, scale_factor)\n",
    "    return np.clip(data * scale, 0, 1)\n",
    "\n",
    "def generate_random_curves(data, sigma=0.078, knot=2):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    xx = np.linspace(0, data.shape[0] - 1, knot + 2)\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, data.shape[1]))\n",
    "    x_range = np.arange(data.shape[0])\n",
    "    augmented_data = np.zeros_like(data)\n",
    "    for i in range(data.shape[1]):\n",
    "        cs = CubicSpline(xx, yy[:, i])\n",
    "        augmented_data[:, i] = data[:, i] * cs(x_range)\n",
    "    return np.clip(augmented_data, 0, 1)\n",
    "\n",
    "def augment_sample(sample):\n",
    "    sample = add_noise(sample)\n",
    "    sample = scale(sample)\n",
    "    sample = generate_random_curves(sample)\n",
    "    return sample\n",
    "\n",
    "def augment_dataset(X, y, target_size=10000):\n",
    "    num_original = len(X)\n",
    "    num_augmented = target_size - num_original\n",
    "\n",
    "    indices = np.random.randint(0, num_original, size=num_augmented)\n",
    "\n",
    "    def process_sample(idx):\n",
    "        augmented_x = augment_sample(X[idx])\n",
    "        return augmented_x, y[idx]\n",
    "\n",
    "    from joblib import Parallel, delayed\n",
    "    augmented_samples = Parallel(n_jobs=-1)(\n",
    "        delayed(process_sample)(idx) for idx in indices\n",
    "    )\n",
    "\n",
    "    augmented_X = np.array([sample[0] for sample in augmented_samples])\n",
    "    augmented_y = np.array([sample[1] for sample in augmented_samples])\n",
    "    return np.vstack((X, augmented_X)), np.vstack((y, augmented_y))\n",
    "\n",
    "# Function to split hand data\n",
    "def split_hand_data(X):\n",
    "    hand1_data = X[:, :, :63]\n",
    "    hand2_data = X[:, :, 63:]\n",
    "    return hand1_data, hand2_data\n",
    "\n",
    "# Function to load or create datasets for each subject\n",
    "def load_or_create_subject_datasets(X, y, subject_number, split_indices, data_folder):\n",
    "    # Determine file paths\n",
    "    X_original_path = os.path.join(data_folder, f'X_subject{subject_number}_original.npy')\n",
    "    y_original_path = os.path.join(data_folder, f'y_subject{subject_number}_original.npy')\n",
    "    X_aug_path = os.path.join(data_folder, f'X_subject{subject_number}_aug.npy')\n",
    "    y_aug_path = os.path.join(data_folder, f'y_subject{subject_number}_aug.npy')\n",
    "\n",
    "    # Check if files exist\n",
    "    files_exist = all(os.path.exists(p) for p in [X_original_path, y_original_path, X_aug_path, y_aug_path])\n",
    "\n",
    "    if files_exist:\n",
    "        print(f\"Loading datasets for Subject {subject_number} from disk.\")\n",
    "        # Load datasets\n",
    "        X_subject_original = np.load(X_original_path)\n",
    "        y_subject_original = np.load(y_original_path)\n",
    "        X_subject_aug = np.load(X_aug_path)\n",
    "        y_subject_aug = np.load(y_aug_path)\n",
    "    else:\n",
    "        print(f\"Creating and saving datasets for Subject {subject_number}.\")\n",
    "        # Split data for the subject\n",
    "        start_idx, end_idx = split_indices\n",
    "        X_subject_original = X[start_idx:end_idx]\n",
    "        y_subject_original = y[start_idx:end_idx]\n",
    "        # Save original data\n",
    "        np.save(X_original_path, X_subject_original)\n",
    "        np.save(y_original_path, y_subject_original)\n",
    "        # Augment data\n",
    "        X_subject_aug, y_subject_aug = augment_dataset(X_subject_original, y_subject_original, 10000)\n",
    "        # Save augmented data\n",
    "        np.save(X_aug_path, X_subject_aug)\n",
    "        np.save(y_aug_path, y_subject_aug)\n",
    "\n",
    "    return (X_subject_original, y_subject_original), (X_subject_aug, y_subject_aug)\n",
    "\n",
    "input_shape_hand = (30, 63)  # Use full sequence length of 30\n",
    "\n",
    "def prepare_datasets(train_subjects, test_subject):\n",
    "    X_train_full = np.vstack([subject_datasets[train][\"aug\"][0] for train in train_subjects])\n",
    "    y_train = np.vstack([subject_datasets[train][\"aug\"][1] for train in train_subjects])\n",
    "    X_test_full, y_test = subject_datasets[test_subject][\"original\"]\n",
    "    X_val_full, X_test_full, y_val, y_test = train_test_split(X_test_full, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Use full sequence length of 30\n",
    "    X_train_full = X_train_full[:, :, :]\n",
    "    X_val_full = X_val_full[:, :, :]\n",
    "    X_test_full = X_test_full[:, :, :]\n",
    "\n",
    "    # Split hand data for both hands\n",
    "    X_train_hand1, X_train_hand2 = split_hand_data(X_train_full)\n",
    "    X_val_hand1, X_val_hand2 = split_hand_data(X_val_full)\n",
    "    X_test_hand1, X_test_hand2 = split_hand_data(X_test_full)\n",
    "\n",
    "    return (X_train_hand1, X_train_hand2, y_train,\n",
    "            X_val_hand1, X_val_hand2, y_val,\n",
    "            X_test_hand1, X_test_hand2, y_test)\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1, name_prefix=\"\"):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln1\")(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha\"\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout, name=f\"{name_prefix}_dropout\")(x)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add\")([x, inputs])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln2\")(x)\n",
    "    x_ff = layers.Dense(ff_dim, activation='gelu', name=f\"{name_prefix}_dense1\")(x)\n",
    "    x_ff = layers.Dropout(dropout, name=f\"{name_prefix}_dropout_ff\")(x_ff)\n",
    "    x_ff = layers.Dense(inputs.shape[-1], name=f\"{name_prefix}_dense2\")(x_ff)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add_ff\")([x_ff, x])\n",
    "    return x\n",
    "\n",
    "def build_classification_model(input_shape_hand, head_size, num_heads, ff_dim, num_encoder_blocks, mlp_units, dropout=0.1):\n",
    "    # Two separate inputs: left hand (hand1_input) and right hand (hand2_input)\n",
    "    # Each hand has shape (30,63)\n",
    "    hand1_input = tf.keras.Input(shape=input_shape_hand, name=\"hand1_input\")\n",
    "    hand2_input = tf.keras.Input(shape=input_shape_hand, name=\"hand2_input\")\n",
    "\n",
    "    # Pass each hand through the specified number of encoder blocks\n",
    "    hand1_encoded = hand1_input\n",
    "    hand2_encoded = hand2_input\n",
    "    for i in range(num_encoder_blocks):\n",
    "        hand1_encoded = transformer_encoder(\n",
    "            hand1_encoded, head_size, num_heads, ff_dim, dropout, name_prefix=f\"hand1_enc_{i}\"\n",
    "        )\n",
    "        hand2_encoded = transformer_encoder(\n",
    "            hand2_encoded, head_size, num_heads, ff_dim, dropout, name_prefix=f\"hand2_enc_{i}\"\n",
    "        )\n",
    "\n",
    "    # Concatenate encoder outputs along the feature dimension\n",
    "    concatenated_output = layers.Concatenate(name=\"encoder_concat\")([hand1_encoded, hand2_encoded])\n",
    "    # concatenated_output shape: (batch_size, 30, 126)\n",
    "\n",
    "    # Add a BiLSTM layer after the concatenation\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=False), name=\"bilstm\")(concatenated_output)\n",
    "\n",
    "    # Add two Dense layers after BiLSTM\n",
    "    for idx, units in enumerate(mlp_units):\n",
    "        x = layers.Dense(units, activation='gelu', name=f\"mlp_dense_{idx}\")(x)\n",
    "        x = layers.Dropout(dropout, name=f\"mlp_dropout_{idx}\")(x)\n",
    "\n",
    "    # Output layer for classification (9 classes)\n",
    "    output = layers.Dense(9, activation='softmax', name=\"classification_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[hand1_input, hand2_input], outputs=output, name=\"classification_model\")\n",
    "    return model\n",
    "\n",
    "# Load the labeled dataset\n",
    "X = np.load('NEW_DATASET/dataset_20240709_X_3096.npy')\n",
    "y = np.load('NEW_DATASET/dataset_20240709_y_3096.npy')\n",
    "\n",
    "# Create data folder if it doesn't exist\n",
    "data_folder = 'processed_datasets'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "# Split indices for the subjects\n",
    "split_60 = int(0.60 * len(X))\n",
    "split_80 = int(0.80 * len(X))\n",
    "\n",
    "# Load or create datasets for subjects\n",
    "(X_subject1, y_subject1), (X_subject1_aug, y_subject1_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=1, split_indices=(0, split_60), data_folder=data_folder)\n",
    "\n",
    "(X_subject2, y_subject2), (X_subject2_aug, y_subject2_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=2, split_indices=(split_60, split_80), data_folder=data_folder)\n",
    "\n",
    "(X_subject3, y_subject3), (X_subject3_aug, y_subject3_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=3, split_indices=(split_80, len(X)), data_folder=data_folder)\n",
    "\n",
    "# Now, store datasets in a dictionary\n",
    "subject_datasets = {\n",
    "    1: {\"aug\": (X_subject1_aug, y_subject1_aug), \"original\": (X_subject1, y_subject1)},\n",
    "    2: {\"aug\": (X_subject2_aug, y_subject2_aug), \"original\": (X_subject2, y_subject2)},\n",
    "    3: {\"aug\": (X_subject3_aug, y_subject3_aug), \"original\": (X_subject3, y_subject3)}\n",
    "}\n",
    "\n",
    "# Define hyperparameters for fine-tuning\n",
    "fine_tune_hyperparameters = {\n",
    "    'head_size': 64,\n",
    "    'num_heads': 4,\n",
    "    'ff_dim': 64,\n",
    "    'num_encoder_blocks': 4,  # same as pretraining\n",
    "    'mlp_units': [64, 32],    # after BiLSTM\n",
    "    'dropout': 0.1,\n",
    "    'learning_rate': 5e-4,\n",
    "    'batch_size': 4096,\n",
    "    'epochs': 200\n",
    "}\n",
    "\n",
    "# Prepare the combinations\n",
    "combinations = [\n",
    "    {'train_subjects': [1, 2], 'test_subject': 3},\n",
    "    {'train_subjects': [1, 3], 'test_subject': 2},\n",
    "    {'train_subjects': [2, 3], 'test_subject': 1}\n",
    "]\n",
    "\n",
    "# Initialize lists to store the metrics\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "for idx, combo in enumerate(combinations):\n",
    "    print(f\"\\nRunning combination {idx+1}: Train on subjects {combo['train_subjects']}, Test on subject {combo['test_subject']}\")\n",
    "\n",
    "    train_subjects = combo['train_subjects']\n",
    "    test_subject = combo['test_subject']\n",
    "\n",
    "    # Prepare datasets\n",
    "    X_train_hand1, X_train_hand2, y_train, \\\n",
    "    X_val_hand1, X_val_hand2, y_val, \\\n",
    "    X_test_hand1, X_test_hand2, y_test = prepare_datasets(train_subjects, test_subject)\n",
    "\n",
    "    # Build the classification model\n",
    "    classification_model = build_classification_model(\n",
    "        input_shape_hand=input_shape_hand,\n",
    "        head_size=fine_tune_hyperparameters['head_size'],\n",
    "        num_heads=fine_tune_hyperparameters['num_heads'],\n",
    "        ff_dim=fine_tune_hyperparameters['ff_dim'],\n",
    "        num_encoder_blocks=fine_tune_hyperparameters['num_encoder_blocks'],\n",
    "        mlp_units=fine_tune_hyperparameters['mlp_units'],\n",
    "        dropout=fine_tune_hyperparameters['dropout']\n",
    "    )\n",
    "\n",
    "    # Load pretrained encoder weights\n",
    "    classification_model.load_weights('encoder_pretrained_weights.h5', by_name=True)\n",
    "    print(\"Pretrained encoder weights loaded.\")\n",
    "\n",
    "    # Freeze encoder layers and allow others to train\n",
    "    \"\"\"\n",
    "    for layer in classification_model.layers:\n",
    "        # If the layer name starts with \"hand1_enc_\" or \"hand2_enc_\", freeze it\n",
    "        if layer.name.startswith(\"hand1_enc_\") or layer.name.startswith(\"hand2_enc_\"):\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0)\n",
    "\n",
    "    # Compile the model\n",
    "    classification_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Define the learning rate schedule function\n",
    "    def lr_schedule(epoch):\n",
    "        warmup_epochs = 20\n",
    "        total_epochs = fine_tune_hyperparameters['epochs']\n",
    "        initial_lr = fine_tune_hyperparameters['learning_rate']\n",
    "        min_lr = 1e-8\n",
    "\n",
    "        if epoch < warmup_epochs:\n",
    "            # Linear warm-up\n",
    "            lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            # Linear decay\n",
    "            decay_epochs = total_epochs - warmup_epochs\n",
    "            lr = initial_lr - (initial_lr - min_lr) * (epoch - warmup_epochs + 1) / decay_epochs\n",
    "            lr = max(lr, min_lr)\n",
    "        return lr\n",
    "\n",
    "    from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "    # Create the LearningRateScheduler callback\n",
    "    callbacks = [LearningRateScheduler(lr_schedule, verbose=0)]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((X_train_hand1, X_train_hand2), y_train)\n",
    "    ).shuffle(10000).batch(fine_tune_hyperparameters['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((X_val_hand1, X_val_hand2), y_val)\n",
    "    ).batch(fine_tune_hyperparameters['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    history = classification_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=fine_tune_hyperparameters['epochs'],\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ((X_test_hand1, X_test_hand2), y_test)\n",
    "    ).batch(fine_tune_hyperparameters['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_probs = classification_model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Get true labels\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Evaluate the model to get loss and accuracy\n",
    "    test_loss, test_accuracy = classification_model.evaluate(test_dataset, verbose=0)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Collect the metrics\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate and print the averages\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "avg_test_accuracy = np.mean(test_accuracies)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "print(f'\\nAverage Test Loss: {avg_test_loss:.4f}')\n",
    "print(f'Average Test Accuracy: {avg_test_accuracy:.4f}')\n",
    "print(f'Average F1 Score: {avg_f1_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd2685-fe75-477d-9197-376c17085553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pretrain with two bilstms and feed to decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe919dfa-013a-4c09-8942-3de394da5eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Memory growth enabled for GPU devices.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow')\n",
    "\n",
    "# Load the unlabeled datasets\n",
    "X_unlabeled1 = np.load('NEW_DATASET/scaled_interhand2.6m.npy')  # (num_samples1, 30, 126)\n",
    "X_unlabeled2 = np.load('NEW_DATASET/hsosunlabeled.npy')         # (num_samples2, 30, 126)\n",
    "\n",
    "# Concatenate the arrays\n",
    "X_unlabeled = np.concatenate((X_unlabeled1, X_unlabeled2), axis=0)\n",
    "print(\"Shape of X_unlabeled:\", X_unlabeled.shape)\n",
    "\n",
    "# Choose which hand to pretrain on (0 for left/hand1, 1 for right/hand2)\n",
    "hand_index = 0\n",
    "if hand_index == 0:\n",
    "    X_hand = X_unlabeled[:, :, :63]  # (num_samples, 30, 63)\n",
    "    hand_name = \"hand1\"\n",
    "else:\n",
    "    X_hand = X_unlabeled[:, :, 63:]  # (num_samples, 30, 63)\n",
    "    hand_name = \"hand2\"\n",
    "\n",
    "# Split into inputs and outputs for next-frame prediction\n",
    "X_input = X_hand[:, :15, :]   # (num_samples, 15, 63)\n",
    "y_output = X_hand[:, 15:, :]  # (num_samples, 15, 63)\n",
    "\n",
    "# Prepare decoder inputs by shifting y_output (just for reference)\n",
    "decoder_inputs = np.zeros_like(y_output)\n",
    "decoder_inputs[:, 1:, :] = y_output[:, :-1, :]\n",
    "\n",
    "print(\"Shape of X_input (encoder input):\", X_input.shape)\n",
    "print(\"Shape of decoder_inputs:\", decoder_inputs.shape)\n",
    "print(\"Shape of y_output (target):\", y_output.shape)\n",
    "\n",
    "def transformer_decoder_block(inputs, encoder_output, head_size, num_heads, ff_dim, dropout=0.1, name_prefix=\"\"):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln1\")(inputs)\n",
    "    x1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha1\"\n",
    "    )(x, x)\n",
    "    x1 = layers.Dropout(dropout, name=f\"{name_prefix}_dropout1\")(x1)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add1\")([x1, inputs])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln2\")(x)\n",
    "    x2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha2\"\n",
    "    )(x, encoder_output)\n",
    "    x2 = layers.Dropout(dropout, name=f\"{name_prefix}_dropout2\")(x2)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add2\")([x2, x])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln3\")(x)\n",
    "    x_ff = layers.Dense(ff_dim, activation='gelu', name=f\"{name_prefix}_dense1_ff\")(x)\n",
    "    x_ff = layers.Dropout(dropout, name=f\"{name_prefix}_dropout_ff\")(x_ff)\n",
    "    x_ff = layers.Dense(inputs.shape[-1], name=f\"{name_prefix}_dense2_ff\")(x_ff)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add_ff\")([x_ff, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_pretraining_model(input_shape, head_size, num_heads, ff_dim, dropout=0.1, num_decoders=1):\n",
    "    left_input = tf.keras.Input(shape=input_shape, name=\"left_hand_input\")\n",
    "    right_input = tf.keras.Input(shape=input_shape, name=\"right_hand_input\")\n",
    "\n",
    "    # BiLSTMs\n",
    "    left_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        name=\"left_bilstm\"\n",
    "    )(left_input)\n",
    "\n",
    "    right_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        name=\"right_bilstm\"\n",
    "    )(right_input)\n",
    "\n",
    "    decoder_input = left_bilstm_output\n",
    "    encoder_output = right_bilstm_output\n",
    "\n",
    "    x = decoder_input\n",
    "    for i in range(num_decoders):\n",
    "        x = transformer_decoder_block(\n",
    "            x, encoder_output,\n",
    "            head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout,\n",
    "            name_prefix=f\"decoder_block_{i}\"\n",
    "        )\n",
    "\n",
    "    output = layers.TimeDistributed(\n",
    "        layers.Dense(126, activation='linear'),\n",
    "        name=\"output\"\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[left_input, right_input], outputs=output, name=\"pretraining_model\")\n",
    "    return model\n",
    "\n",
    "# Define Hyperparameters\n",
    "pretrain_hyperparameters = {\n",
    "    'head_size': 64,\n",
    "    'num_heads': 4,\n",
    "    'ff_dim': 64,\n",
    "    'dropout': 0.1,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 2048,\n",
    "    'epochs': 200,\n",
    "    'num_decoders': 1  # Adjust this as needed\n",
    "}\n",
    "\n",
    "input_shape = (15, 63)\n",
    "pretraining_model = build_pretraining_model(\n",
    "    input_shape=input_shape,\n",
    "    head_size=pretrain_hyperparameters['head_size'],\n",
    "    num_heads=pretrain_hyperparameters['num_heads'],\n",
    "    ff_dim=pretrain_hyperparameters['ff_dim'],\n",
    "    dropout=pretrain_hyperparameters['dropout'],\n",
    "    num_decoders=pretrain_hyperparameters['num_decoders']\n",
    ")\n",
    "\n",
    "# Freeze all layers except the BiLSTM layers\n",
    "for layer in pretraining_model.layers:\n",
    "    if layer.name not in [\"left_bilstm\", \"right_bilstm\"]:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0)\n",
    "pretraining_model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    warmup_epochs = 20\n",
    "    total_epochs = pretrain_hyperparameters['epochs']\n",
    "    initial_lr = pretrain_hyperparameters['learning_rate']\n",
    "    min_lr = 1e-8\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        decay_epochs = total_epochs - warmup_epochs\n",
    "        lr = initial_lr - (initial_lr - min_lr) * (epoch - warmup_epochs + 1) / decay_epochs\n",
    "        lr = max(lr, min_lr)\n",
    "    print(f\"Epoch {epoch+1}: Learning rate is {lr:.8f}\")\n",
    "    return lr\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "callbacks = [LearningRateScheduler(lr_schedule, verbose=1)]\n",
    "\n",
    "# Prepare target data\n",
    "y_output_full = np.concatenate([y_output, y_output], axis=-1)\n",
    "\n",
    "pretraining_model.fit(\n",
    "    [X_input, X_input],\n",
    "    y_output_full,\n",
    "    epochs=pretrain_hyperparameters['epochs'],\n",
    "    batch_size=pretrain_hyperparameters['batch_size'],\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extract only the BiLSTM weights\n",
    "left_bilstm_layer = pretraining_model.get_layer('left_bilstm')\n",
    "right_bilstm_layer = pretraining_model.get_layer('right_bilstm')\n",
    "\n",
    "left_weights = left_bilstm_layer.get_weights()\n",
    "right_weights = right_bilstm_layer.get_weights()\n",
    "\n",
    "# Save only the BiLSTM weights\n",
    "np.savez('bilstm_pretrained_weights.npz',\n",
    "         left_bilstm=left_weights,\n",
    "         right_bilstm=right_weights)\n",
    "\n",
    "print(\"BiLSTM pretrained weights saved independently of the decoder block.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7efea-b653-4765-8d3f-51d41bdea1fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Use pretrained weights of double bilstm to decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0733763-8872-4143-ac59-19518cd0ef0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth enabled for GPU devices.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "\n",
    "def add_noise(data, noise_factor=0.0185):\n",
    "    noise = np.random.randn(*data.shape) * noise_factor\n",
    "    return np.clip(data + noise, 0, 1)\n",
    "\n",
    "def scale(data, scale_factor=0.1605):\n",
    "    scale = 1 + np.random.uniform(-scale_factor, scale_factor)\n",
    "    return np.clip(data * scale, 0, 1)\n",
    "\n",
    "def generate_random_curves(data, sigma=0.078, knot=2):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    xx = np.linspace(0, data.shape[0] - 1, knot + 2)\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, data.shape[1]))\n",
    "    x_range = np.arange(data.shape[0])\n",
    "    augmented_data = np.zeros_like(data)\n",
    "    for i in range(data.shape[1]):\n",
    "        cs = CubicSpline(xx, yy[:, i])\n",
    "        augmented_data[:, i] = data[:, i] * cs(x_range)\n",
    "    return np.clip(augmented_data, 0, 1)\n",
    "\n",
    "def augment_sample(sample):\n",
    "    sample = add_noise(sample)\n",
    "    sample = scale(sample)\n",
    "    sample = generate_random_curves(sample)\n",
    "    return sample\n",
    "\n",
    "def augment_dataset(X, y, target_size=10000):\n",
    "    num_original = len(X)\n",
    "    num_augmented = target_size - num_original\n",
    "    indices = np.random.randint(0, num_original, size=num_augmented)\n",
    "\n",
    "    from joblib import Parallel, delayed\n",
    "    augmented_samples = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda idx: (augment_sample(X[idx]), y[idx]))(idx) for idx in indices\n",
    "    )\n",
    "\n",
    "    augmented_X = np.array([s[0] for s in augmented_samples])\n",
    "    augmented_y = np.array([s[1] for s in augmented_samples])\n",
    "    return np.vstack((X, augmented_X)), np.vstack((y, augmented_y))\n",
    "\n",
    "def split_hand_data(X):\n",
    "    hand1_data = X[:, :, :63]\n",
    "    hand2_data = X[:, :, 63:]\n",
    "    return hand1_data, hand2_data\n",
    "\n",
    "def load_or_create_subject_datasets(X, y, subject_number, split_indices, data_folder):\n",
    "    X_original_path = os.path.join(data_folder, f'X_subject{subject_number}_original.npy')\n",
    "    y_original_path = os.path.join(data_folder, f'y_subject{subject_number}_original.npy')\n",
    "    X_aug_path = os.path.join(data_folder, f'X_subject{subject_number}_aug.npy')\n",
    "    y_aug_path = os.path.join(data_folder, f'y_subject{subject_number}_aug.npy')\n",
    "\n",
    "    files_exist = all(os.path.exists(p) for p in [X_original_path, y_original_path, X_aug_path, y_aug_path])\n",
    "\n",
    "    if files_exist:\n",
    "        print(f\"Loading datasets for Subject {subject_number} from disk.\")\n",
    "        X_subject_original = np.load(X_original_path)\n",
    "        y_subject_original = np.load(y_original_path)\n",
    "        X_subject_aug = np.load(X_aug_path)\n",
    "        y_subject_aug = np.load(y_aug_path)\n",
    "    else:\n",
    "        print(f\"Creating and saving datasets for Subject {subject_number}.\")\n",
    "        start_idx, end_idx = split_indices\n",
    "        X_subject_original = X[start_idx:end_idx]\n",
    "        y_subject_original = y[start_idx:end_idx]\n",
    "        np.save(X_original_path, X_subject_original)\n",
    "        np.save(y_original_path, y_subject_original)\n",
    "        X_subject_aug, y_subject_aug = augment_dataset(X_subject_original, y_subject_original, 10000)\n",
    "        np.save(X_aug_path, X_subject_aug)\n",
    "        np.save(y_aug_path, y_aug_path)\n",
    "\n",
    "    return (X_subject_original, y_subject_original), (X_subject_aug, y_subject_aug)\n",
    "\n",
    "input_shape_hand = (30, 63)\n",
    "\n",
    "def prepare_datasets(train_subjects, test_subject):\n",
    "    X_train_full = np.vstack([subject_datasets[train][\"aug\"][0] for train in train_subjects])\n",
    "    y_train = np.vstack([subject_datasets[train][\"aug\"][1] for train in train_subjects])\n",
    "    X_test_full, y_test = subject_datasets[test_subject][\"original\"]\n",
    "    X_val_full, X_test_full, y_val, y_test = train_test_split(X_test_full, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    X_train_hand1, X_train_hand2 = split_hand_data(X_train_full)\n",
    "    X_val_hand1, X_val_hand2 = split_hand_data(X_val_full)\n",
    "    X_test_hand1, X_test_hand2 = split_hand_data(X_test_full)\n",
    "\n",
    "    return (X_train_hand1, X_train_hand2, y_train,\n",
    "            X_val_hand1, X_val_hand2, y_val,\n",
    "            X_test_hand1, X_test_hand2, y_test)\n",
    "\n",
    "def transformer_decoder_block(inputs, encoder_output, head_size, num_heads, ff_dim, dropout=0.1, name_prefix=\"\"):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln1\")(inputs)\n",
    "    x1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha1\"\n",
    "    )(x, x)\n",
    "    x1 = layers.Dropout(dropout, name=f\"{name_prefix}_dropout1\")(x1)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add1\")([x1, inputs])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln2\")(x)\n",
    "    x2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha2\"\n",
    "    )(x, encoder_output)\n",
    "    x2 = layers.Dropout(dropout, name=f\"{name_prefix}_dropout2\")(x2)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add2\")([x2, x])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln3\")(x)\n",
    "    x_ff = layers.Dense(ff_dim, activation='gelu', name=f\"{name_prefix}_dense1_ff\")(x)\n",
    "    x_ff = layers.Dropout(dropout, name=f\"{name_prefix}_dropout_ff\")(x_ff)\n",
    "    x_ff = layers.Dense(inputs.shape[-1], name=f\"{name_prefix}_dense2_ff\")(x_ff)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add_ff\")([x_ff, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_pretrained_structure(input_shape, head_size, num_heads, ff_dim, dropout=0.1, num_decoders=1):\n",
    "    left_input = tf.keras.Input(shape=input_shape, name=\"left_hand_input\")\n",
    "    right_input = tf.keras.Input(shape=input_shape, name=\"right_hand_input\")\n",
    "\n",
    "    left_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        name=\"left_bilstm\"\n",
    "    )(left_input)\n",
    "\n",
    "    right_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        name=\"right_bilstm\"\n",
    "    )(right_input)\n",
    "\n",
    "    decoder_input = left_bilstm_output\n",
    "    encoder_output = right_bilstm_output\n",
    "\n",
    "    x = decoder_input\n",
    "    for i in range(num_decoders):\n",
    "        x = transformer_decoder_block(\n",
    "            x, encoder_output,\n",
    "            head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout,\n",
    "            name_prefix=f\"decoder_block_{i}\"\n",
    "        )\n",
    "\n",
    "    output = layers.TimeDistributed(\n",
    "        layers.Dense(126, activation='linear'), name=\"output\"\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[left_input, right_input], outputs=output, name=\"pretrained_structure\")\n",
    "    return model\n",
    "\n",
    "def build_classification_model_from_pretrained(pretrained_model, mlp_units=[128,64], dropout=0.1, num_decoders=1):\n",
    "    # Always use \"decoder_block_{num_decoders-1}\"\n",
    "    last_decoder_prefix = f\"decoder_block_{num_decoders-1}\"\n",
    "    decoder_output_layer = pretrained_model.get_layer(last_decoder_prefix + \"_add_ff\")\n",
    "    decoder_output = decoder_output_layer.output\n",
    "\n",
    "    x = layers.Flatten(name=\"flatten\")(decoder_output)\n",
    "    for idx, units in enumerate(mlp_units):\n",
    "        x = layers.Dense(units, activation='gelu', name=f\"mlp_dense_{idx}\")(x)\n",
    "        x = layers.Dropout(dropout, name=f\"mlp_dropout_{idx}\")(x)\n",
    "\n",
    "    output = layers.Dense(9, activation='softmax', name=\"classification_output\")(x)\n",
    "    classification_model = tf.keras.Model(inputs=pretrained_model.inputs, outputs=output, name=\"classification_model\")\n",
    "    return classification_model\n",
    "\n",
    "\n",
    "X = np.load('NEW_DATASET/dataset_20240709_X_3096.npy')\n",
    "y = np.load('NEW_DATASET/dataset_20240709_y_3096.npy')\n",
    "\n",
    "data_folder = 'processed_datasets'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "split_60 = int(0.60 * len(X))\n",
    "split_80 = int(0.80 * len(X))\n",
    "\n",
    "(X_subject1, y_subject1), (X_subject1_aug, y_subject1_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=1, split_indices=(0, split_60), data_folder=data_folder)\n",
    "\n",
    "(X_subject2, y_subject2), (X_subject2_aug, y_subject2_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=2, split_indices=(split_60, split_80), data_folder=data_folder)\n",
    "\n",
    "(X_subject3, y_subject3), (X_subject3_aug, y_subject3_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=3, split_indices=(split_80, len(X)), data_folder=data_folder)\n",
    "\n",
    "subject_datasets = {\n",
    "    1: {\"aug\": (X_subject1_aug, y_subject1_aug), \"original\": (X_subject1, y_subject1)},\n",
    "    2: {\"aug\": (X_subject2_aug, y_subject2_aug), \"original\": (X_subject2, y_subject2)},\n",
    "    3: {\"aug\": (X_subject3_aug, y_subject3_aug), \"original\": (X_subject3, y_subject3)}\n",
    "}\n",
    "\n",
    "# Add num_decoders as a hyperparameter\n",
    "head_sizes = [64]\n",
    "num_heads_list = [4]\n",
    "ff_dims = [128]\n",
    "dropouts = [0.1, 0.2]\n",
    "learning_rates = np.linspace(2e-4, 1e-3, 3) \n",
    "batch_sizes = [2048]\n",
    "epochs = 200\n",
    "num_decoders_list = [1, 2, 3]  # Example values for number of decoders\n",
    "\n",
    "combinations_subjects = [\n",
    "    {'train_subjects': [1, 2], 'test_subject': 3},\n",
    "    {'train_subjects': [1, 3], 'test_subject': 2},\n",
    "    {'train_subjects': [2, 3], 'test_subject': 1}\n",
    "]\n",
    "\n",
    "for head_size in head_sizes:\n",
    "    for num_heads in num_heads_list:\n",
    "        for ff_dim in ff_dims:\n",
    "            for dropout in dropouts:\n",
    "                for learning_rate in learning_rates:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        for num_decoders in num_decoders_list:\n",
    "                            print(\"\\n======================================\")\n",
    "                            print(f\"Testing hyperparameters: head_size={head_size}, num_heads={num_heads}, ff_dim={ff_dim}, dropout={dropout}, lr={learning_rate}, batch_size={batch_size}, epochs={epochs}, num_decoders={num_decoders}\")\n",
    "                            print(\"======================================\\n\")\n",
    "\n",
    "                            test_losses = []\n",
    "                            test_accuracies = []\n",
    "                            f1_scores = []\n",
    "\n",
    "                            for idx, combo in enumerate(combinations_subjects):\n",
    "                                print(f\"\\nRunning combination {idx+1}: Train on subjects {combo['train_subjects']}, Test on subject {combo['test_subject']}\")\n",
    "\n",
    "                                train_subjects = combo['train_subjects']\n",
    "                                test_subject = combo['test_subject']\n",
    "\n",
    "                                X_train_hand1, X_train_hand2, y_train, \\\n",
    "                                X_val_hand1, X_val_hand2, y_val, \\\n",
    "                                X_test_hand1, X_test_hand2, y_test = prepare_datasets(train_subjects, test_subject)\n",
    "\n",
    "                                pretrained_structure = build_pretrained_structure(\n",
    "                                    input_shape_hand,\n",
    "                                    head_size=head_size,\n",
    "                                    num_heads=num_heads,\n",
    "                                    ff_dim=ff_dim,\n",
    "                                    dropout=dropout,\n",
    "                                    num_decoders=num_decoders\n",
    "                                )\n",
    "\n",
    "                                # Load only BiLSTM weights\n",
    "                                bilstm_weights = np.load('bilstm_pretrained_weights.npz', allow_pickle=True)\n",
    "                                left_weights = bilstm_weights['left_bilstm']\n",
    "                                right_weights = bilstm_weights['right_bilstm']\n",
    "\n",
    "                                pretrained_structure.get_layer('left_bilstm').set_weights(left_weights)\n",
    "                                pretrained_structure.get_layer('right_bilstm').set_weights(right_weights)\n",
    "\n",
    "                                print(\"BiLSTM pretrained weights loaded.\")\n",
    "\n",
    "                                classification_model = build_classification_model_from_pretrained(\n",
    "                                    pretrained_structure,\n",
    "                                    mlp_units=[128,64],\n",
    "                                    dropout=dropout,\n",
    "                                    num_decoders=num_decoders\n",
    "                                )\n",
    "\n",
    "                                # BiLSTM weights are already set in pretrained_structure and classification_model shares them\n",
    "                                # Freeze BiLSTM layers so they are NOT trained\n",
    "                                for layer in classification_model.layers:\n",
    "                                    if 'bilstm' in layer.name:\n",
    "                                        layer.trainable = False\n",
    "                                    else:\n",
    "                                        layer.trainable = True\n",
    "\n",
    "                                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0)\n",
    "                                classification_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "                                def lr_schedule(epoch):\n",
    "                                    warmup_epochs = 20\n",
    "                                    total_epochs = epochs\n",
    "                                    initial_lr = learning_rate\n",
    "                                    min_lr = 1e-8\n",
    "\n",
    "                                    if epoch < warmup_epochs:\n",
    "                                        lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "                                    else:\n",
    "                                        decay_epochs = total_epochs - warmup_epochs\n",
    "                                        lr = initial_lr - (initial_lr - min_lr) * (epoch - warmup_epochs + 1) / decay_epochs\n",
    "                                        lr = max(lr, min_lr)\n",
    "                                    return lr\n",
    "\n",
    "                                from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "                                callbacks = [LearningRateScheduler(lr_schedule, verbose=0)]\n",
    "\n",
    "                                train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                    ((X_train_hand1, X_train_hand2), y_train)\n",
    "                                ).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                                val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                    ((X_val_hand1, X_val_hand2), y_val)\n",
    "                                ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                                history = classification_model.fit(\n",
    "                                    train_dataset,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=val_dataset,\n",
    "                                    callbacks=callbacks,\n",
    "                                    verbose=0\n",
    "                                )\n",
    "\n",
    "                                test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                    ((X_test_hand1, X_test_hand2), y_test)\n",
    "                                ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                                y_pred_probs = classification_model.predict(test_dataset)\n",
    "                                y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "                                y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "                                f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "                                test_loss, test_accuracy = classification_model.evaluate(test_dataset, verbose=0)\n",
    "                                print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "                                test_losses.append(test_loss)\n",
    "                                test_accuracies.append(test_accuracy)\n",
    "                                f1_scores.append(f1)\n",
    "\n",
    "                            # Average metrics over three subject combinations\n",
    "                            avg_test_loss = np.mean(test_losses)\n",
    "                            avg_test_accuracy = np.mean(test_accuracies)\n",
    "                            avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                            print(\"\\n*** AVERAGED RESULTS FOR THIS HYPERPARAMETER SET ***\")\n",
    "                            print(f'Average Test Loss: {avg_test_loss:.4f}')\n",
    "                            print(f'Average Test Accuracy: {avg_test_accuracy:.4f}')\n",
    "                            print(f'Average F1 Score: {avg_f1_score:.4f}')\n",
    "                            print(\"****************************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7843356-c4cb-4052-8c3e-cdc53331b577",
   "metadata": {},
   "source": [
    "# Train from scratch with BiLSTM and deocder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6a4bd-732f-4948-847b-b4e23f449d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.set_visible_devices(physical_gpus[0], 'GPU')\n",
    "        print(\"Memory growth enabled for GPU devices.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "\n",
    "def add_noise(data, noise_factor=0.0185):\n",
    "    noise = np.random.randn(*data.shape) * noise_factor\n",
    "    return np.clip(data + noise, 0, 1)\n",
    "\n",
    "def scale(data, scale_factor=0.1605):\n",
    "    scale = 1 + np.random.uniform(-scale_factor, scale_factor)\n",
    "    return np.clip(data * scale, 0, 1)\n",
    "\n",
    "def generate_random_curves(data, sigma=0.078, knot=2):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    xx = np.linspace(0, data.shape[0] - 1, knot + 2)\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, data.shape[1]))\n",
    "    x_range = np.arange(data.shape[0])\n",
    "    augmented_data = np.zeros_like(data)\n",
    "    for i in range(data.shape[1]):\n",
    "        cs = CubicSpline(xx, yy[:, i])\n",
    "        augmented_data[:, i] = data[:, i] * cs(x_range)\n",
    "    return np.clip(augmented_data, 0, 1)\n",
    "\n",
    "def augment_sample(sample):\n",
    "    sample = add_noise(sample)\n",
    "    sample = scale(sample)\n",
    "    sample = generate_random_curves(sample)\n",
    "    return sample\n",
    "\n",
    "def augment_dataset(X, y, target_size=10000):\n",
    "    num_original = len(X)\n",
    "    num_augmented = target_size - num_original\n",
    "    indices = np.random.randint(0, num_original, size=num_augmented)\n",
    "\n",
    "    from joblib import Parallel, delayed\n",
    "    augmented_samples = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda idx: (augment_sample(X[idx]), y[idx]))(idx) for idx in indices\n",
    "    )\n",
    "\n",
    "    augmented_X = np.array([s[0] for s in augmented_samples])\n",
    "    augmented_y = np.array([s[1] for s in augmented_samples])\n",
    "    return np.vstack((X, augmented_X)), np.vstack((y, augmented_y))\n",
    "\n",
    "def split_hand_data(X):\n",
    "    hand1_data = X[:, :, :63]\n",
    "    hand2_data = X[:, :, 63:]\n",
    "    return hand1_data, hand2_data\n",
    "\n",
    "def load_or_create_subject_datasets(X, y, subject_number, split_indices, data_folder):\n",
    "    X_original_path = os.path.join(data_folder, f'X_subject{subject_number}_original.npy')\n",
    "    y_original_path = os.path.join(data_folder, f'y_subject{subject_number}_original.npy')\n",
    "    X_aug_path = os.path.join(data_folder, f'X_subject{subject_number}_aug.npy')\n",
    "    y_aug_path = os.path.join(data_folder, f'y_subject{subject_number}_aug.npy')\n",
    "\n",
    "    files_exist = all(os.path.exists(p) for p in [X_original_path, y_original_path, X_aug_path, y_aug_path])\n",
    "\n",
    "    if files_exist:\n",
    "        print(f\"Loading datasets for Subject {subject_number} from disk.\")\n",
    "        X_subject_original = np.load(X_original_path)\n",
    "        y_subject_original = np.load(y_original_path)\n",
    "        X_subject_aug = np.load(X_aug_path)\n",
    "        y_subject_aug = np.load(y_aug_path)\n",
    "    else:\n",
    "        print(f\"Creating and saving datasets for Subject {subject_number}.\")\n",
    "        start_idx, end_idx = split_indices\n",
    "        X_subject_original = X[start_idx:end_idx]\n",
    "        y_subject_original = y[start_idx:end_idx]\n",
    "        np.save(X_original_path, X_subject_original)\n",
    "        np.save(y_original_path, y_subject_original)\n",
    "        X_subject_aug, y_subject_aug = augment_dataset(X_subject_original, y_subject_original, 10000)\n",
    "        np.save(X_aug_path, X_subject_aug)\n",
    "        np.save(y_aug_path, y_aug_path)\n",
    "\n",
    "    return (X_subject_original, y_subject_original), (X_subject_aug, y_subject_aug)\n",
    "\n",
    "input_shape_hand = (30, 63)\n",
    "\n",
    "def prepare_datasets(train_subjects, test_subject):\n",
    "    X_train_full = np.vstack([subject_datasets[train][\"aug\"][0] for train in train_subjects])\n",
    "    y_train = np.vstack([subject_datasets[train][\"aug\"][1] for train in train_subjects])\n",
    "    X_test_full, y_test = subject_datasets[test_subject][\"original\"]\n",
    "    X_val_full, X_test_full, y_val, y_test = train_test_split(X_test_full, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    X_train_hand1, X_train_hand2 = split_hand_data(X_train_full)\n",
    "    X_val_hand1, X_val_hand2 = split_hand_data(X_val_full)\n",
    "    X_test_hand1, X_test_hand2 = split_hand_data(X_test_full)\n",
    "\n",
    "    return (X_train_hand1, X_train_hand2, y_train,\n",
    "            X_val_hand1, X_val_hand2, y_val,\n",
    "            X_test_hand1, X_test_hand2, y_test)\n",
    "\n",
    "def transformer_decoder_block(inputs, encoder_output, head_size, num_heads, ff_dim, dropout=0.1, name_prefix=\"\"):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln1\")(inputs)\n",
    "    x1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha1\"\n",
    "    )(x, x)\n",
    "    x1 = layers.Dropout(dropout, name=f\"{name_prefix}_dropout1\")(x1)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add1\")([x1, inputs])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln2\")(x)\n",
    "    x2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout, name=f\"{name_prefix}_mha2\"\n",
    "    )(x, encoder_output)\n",
    "    x2 = layers.Dropout(dropout, name=f\"{name_prefix}_dropout2\")(x2)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add2\")([x2, x])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_ln3\")(x)\n",
    "    x_ff = layers.Dense(ff_dim, activation='gelu', name=f\"{name_prefix}_dense1_ff\")(x)\n",
    "    x_ff = layers.Dropout(dropout, name=f\"{name_prefix}_dropout_ff\")(x_ff)\n",
    "    x_ff = layers.Dense(inputs.shape[-1], name=f\"{name_prefix}_dense2_ff\")(x_ff)\n",
    "    x = layers.Add(name=f\"{name_prefix}_add_ff\")([x_ff, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_pretrained_structure(input_shape, head_size, num_heads, ff_dim, dropout=0.1, num_decoders=1, bilstm_unit_size=256):\n",
    "    left_input = tf.keras.Input(shape=input_shape, name=\"left_hand_input\")\n",
    "    right_input = tf.keras.Input(shape=input_shape, name=\"right_hand_input\")\n",
    "\n",
    "    left_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(bilstm_unit_size, return_sequences=True),\n",
    "        name=\"left_bilstm\"\n",
    "    )(left_input)\n",
    "\n",
    "    right_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(bilstm_unit_size, return_sequences=True),\n",
    "        name=\"right_bilstm\"\n",
    "    )(right_input)\n",
    "\n",
    "    decoder_input = left_bilstm_output\n",
    "    encoder_output = right_bilstm_output\n",
    "\n",
    "    x = decoder_input\n",
    "    for i in range(num_decoders):\n",
    "        x = transformer_decoder_block(\n",
    "            x, encoder_output,\n",
    "            head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout,\n",
    "            name_prefix=f\"decoder_block_{i}\"\n",
    "        )\n",
    "\n",
    "    output = layers.TimeDistributed(\n",
    "        layers.Dense(126, activation='linear'), name=\"output\"\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[left_input, right_input], outputs=output, name=\"pretrained_structure\")\n",
    "    return model\n",
    "\n",
    "def build_classification_model_from_pretrained(pretrained_model, mlp_units=[128,64], dropout=0.1, num_decoders=1):\n",
    "    # Always use \"decoder_block_{num_decoders-1}\"\n",
    "    last_decoder_prefix = f\"decoder_block_{num_decoders-1}\"\n",
    "    decoder_output_layer = pretrained_model.get_layer(last_decoder_prefix + \"_add_ff\")\n",
    "    decoder_output = decoder_output_layer.output\n",
    "\n",
    "    x = layers.Flatten(name=\"flatten\")(decoder_output)\n",
    "    for idx, units in enumerate(mlp_units):\n",
    "        x = layers.Dense(units, activation='gelu', name=f\"mlp_dense_{idx}\")(x)\n",
    "        x = layers.Dropout(dropout, name=f\"mlp_dropout_{idx}\")(x)\n",
    "\n",
    "    output = layers.Dense(9, activation='softmax', name=\"classification_output\")(x)\n",
    "    classification_model = tf.keras.Model(inputs=pretrained_model.inputs, outputs=output, name=\"classification_model\")\n",
    "    return classification_model\n",
    "\n",
    "X = np.load('NEW_DATASET/dataset_20240709_X_3096.npy')\n",
    "y = np.load('NEW_DATASET/dataset_20240709_y_3096.npy')\n",
    "\n",
    "data_folder = 'processed_datasets'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "split_60 = int(0.60 * len(X))\n",
    "split_80 = int(0.80 * len(X))\n",
    "\n",
    "(X_subject1, y_subject1), (X_subject1_aug, y_subject1_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=1, split_indices=(0, split_60), data_folder=data_folder)\n",
    "\n",
    "(X_subject2, y_subject2), (X_subject2_aug, y_subject2_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=2, split_indices=(split_60, split_80), data_folder=data_folder)\n",
    "\n",
    "(X_subject3, y_subject3), (X_subject3_aug, y_subject3_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=3, split_indices=(split_80, len(X)), data_folder=data_folder)\n",
    "\n",
    "subject_datasets = {\n",
    "    1: {\"aug\": (X_subject1_aug, y_subject1_aug), \"original\": (X_subject1, y_subject1)},\n",
    "    2: {\"aug\": (X_subject2_aug, y_subject2_aug), \"original\": (X_subject2, y_subject2)},\n",
    "    3: {\"aug\": (X_subject3_aug, y_subject3_aug), \"original\": (X_subject3, y_subject3)}\n",
    "}\n",
    "\n",
    "# Add bilstm_unit_size as a hyperparameter\n",
    "bilstm_unit_sizes = [128, 256]  # Example sizes to tune\n",
    "head_sizes = [64,128]\n",
    "num_heads_list = [4]\n",
    "ff_dims = [64]\n",
    "dropouts = [0.2]\n",
    "learning_rates = np.linspace(2e-4, 8e-4, 3) \n",
    "batch_sizes = [2048]\n",
    "epochs = 200\n",
    "num_decoders_list = [1]  # Example values for number of decoders\n",
    "\n",
    "combinations_subjects = [\n",
    "    {'train_subjects': [1, 2], 'test_subject': 3},\n",
    "    {'train_subjects': [1, 3], 'test_subject': 2},\n",
    "    {'train_subjects': [2, 3], 'test_subject': 1}\n",
    "]\n",
    "\n",
    "for bilstm_unit_size in bilstm_unit_sizes:\n",
    "    for head_size in head_sizes:\n",
    "        for num_heads in num_heads_list:\n",
    "            for ff_dim in ff_dims:\n",
    "                for dropout in dropouts:\n",
    "                    for learning_rate in learning_rates:\n",
    "                        for batch_size in batch_sizes:\n",
    "                            for num_decoders in num_decoders_list:\n",
    "                                print(\"\\n======================================\")\n",
    "                                print(f\"Testing hyperparameters: bilstm_unit_size={bilstm_unit_size}, head_size={head_size}, num_heads={num_heads}, ff_dim={ff_dim}, dropout={dropout}, lr={learning_rate}, batch_size={batch_size}, epochs={epochs}, num_decoders={num_decoders}\")\n",
    "                                print(\"======================================\\n\")\n",
    "\n",
    "                                test_losses = []\n",
    "                                test_accuracies = []\n",
    "                                f1_scores = []\n",
    "\n",
    "                                for idx, combo in enumerate(combinations_subjects):\n",
    "                                    print(f\"\\nRunning combination {idx+1}: Train on subjects {combo['train_subjects']}, Test on subject {combo['test_subject']}\")\n",
    "\n",
    "                                    train_subjects = combo['train_subjects']\n",
    "                                    test_subject = combo['test_subject']\n",
    "\n",
    "                                    X_train_hand1, X_train_hand2, y_train, \\\n",
    "                                    X_val_hand1, X_val_hand2, y_val, \\\n",
    "                                    X_test_hand1, X_test_hand2, y_test = prepare_datasets(train_subjects, test_subject)\n",
    "\n",
    "                                    pretrained_structure = build_pretrained_structure(\n",
    "                                        input_shape_hand,\n",
    "                                        head_size=head_size,\n",
    "                                        num_heads=num_heads,\n",
    "                                        ff_dim=ff_dim,\n",
    "                                        dropout=dropout,\n",
    "                                        num_decoders=num_decoders,\n",
    "                                        bilstm_unit_size=bilstm_unit_size\n",
    "                                    )\n",
    "\n",
    "                                    # Train from scratch: no pretrained weights, no freezing\n",
    "                                    classification_model = build_classification_model_from_pretrained(\n",
    "                                        pretrained_structure,\n",
    "                                        mlp_units=[128,64],\n",
    "                                        dropout=dropout,\n",
    "                                        num_decoders=num_decoders\n",
    "                                    )\n",
    "\n",
    "                                    for layer in classification_model.layers:\n",
    "                                        layer.trainable = True\n",
    "\n",
    "                                    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0)\n",
    "                                    classification_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "                                    def lr_schedule(epoch):\n",
    "                                        warmup_epochs = 20\n",
    "                                        total_epochs = epochs\n",
    "                                        initial_lr = learning_rate\n",
    "                                        min_lr = 1e-8\n",
    "\n",
    "                                        if epoch < warmup_epochs:\n",
    "                                            lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "                                        else:\n",
    "                                            decay_epochs = total_epochs - warmup_epochs\n",
    "                                            lr = initial_lr - (initial_lr - min_lr) * (epoch - warmup_epochs + 1) / decay_epochs\n",
    "                                            lr = max(lr, min_lr)\n",
    "                                        return lr\n",
    "\n",
    "                                    from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "                                    callbacks = [LearningRateScheduler(lr_schedule, verbose=0)]\n",
    "\n",
    "                                    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                        ((X_train_hand1, X_train_hand2), y_train)\n",
    "                                    ).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                                    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                        ((X_val_hand1, X_val_hand2), y_val)\n",
    "                                    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                                    history = classification_model.fit(\n",
    "                                        train_dataset,\n",
    "                                        epochs=epochs,\n",
    "                                        validation_data=val_dataset,\n",
    "                                        callbacks=callbacks,\n",
    "                                        verbose=0\n",
    "                                    )\n",
    "\n",
    "                                    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                        ((X_test_hand1, X_test_hand2), y_test)\n",
    "                                    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                                    y_pred_probs = classification_model.predict(test_dataset)\n",
    "                                    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "                                    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "                                    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "                                    test_loss, test_accuracy = classification_model.evaluate(test_dataset, verbose=0)\n",
    "                                    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "                                    test_losses.append(test_loss)\n",
    "                                    test_accuracies.append(test_accuracy)\n",
    "                                    f1_scores.append(f1)\n",
    "\n",
    "                                # Average metrics over three subject combinations\n",
    "                                avg_test_loss = np.mean(test_losses)\n",
    "                                avg_test_accuracy = np.mean(test_accuracies)\n",
    "                                avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "                                print(\"\\n*** AVERAGED RESULTS FOR THIS HYPERPARAMETER SET ***\")\n",
    "                                print(f'Average Test Loss: {avg_test_loss:.4f}')\n",
    "                                print(f'Average Test Accuracy: {avg_test_accuracy:.4f}')\n",
    "                                print(f'Average F1 Score: {avg_f1_score:.4f}')\n",
    "                                print(\"****************************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e2f4f-8071-4bdc-9dc7-1fc8e7ef5d4b",
   "metadata": {},
   "source": [
    "# Two BiLSTMs, one flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeda877-5a93-4253-a59f-be81b2b1d6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "def add_noise(data, noise_factor=0.0185):\n",
    "    noise = np.random.randn(*data.shape) * noise_factor\n",
    "    return np.clip(data + noise, 0, 1)\n",
    "\n",
    "def scale(data, scale_factor=0.1605):\n",
    "    scale = 1 + np.random.uniform(-scale_factor, scale_factor)\n",
    "    return np.clip(data * scale, 0, 1)\n",
    "\n",
    "def generate_random_curves(data, sigma=0.078, knot=2):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    xx = np.linspace(0, data.shape[0] - 1, knot + 2)\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, data.shape[1]))\n",
    "    x_range = np.arange(data.shape[0])\n",
    "    augmented_data = np.zeros_like(data)\n",
    "    for i in range(data.shape[1]):\n",
    "        cs = CubicSpline(xx, yy[:, i])\n",
    "        augmented_data[:, i] = data[:, i] * cs(x_range)\n",
    "    return np.clip(augmented_data, 0, 1)\n",
    "\n",
    "def augment_sample(sample):\n",
    "    sample = add_noise(sample)\n",
    "    sample = scale(sample)\n",
    "    sample = generate_random_curves(sample)\n",
    "    return sample\n",
    "\n",
    "def augment_dataset(X, y, target_size=10000):\n",
    "    num_original = len(X)\n",
    "    num_augmented = target_size - num_original\n",
    "    indices = np.random.randint(0, num_original, size=num_augmented)\n",
    "\n",
    "    from joblib import Parallel, delayed\n",
    "    augmented_samples = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda idx: (augment_sample(X[idx]), y[idx]))(idx) for idx in indices\n",
    "    )\n",
    "\n",
    "    augmented_X = np.array([s[0] for s in augmented_samples])\n",
    "    augmented_y = np.array([s[1] for s in augmented_samples])\n",
    "    return np.vstack((X, augmented_X)), np.vstack((y, augmented_y))\n",
    "\n",
    "def split_hand_data(X):\n",
    "    hand1_data = X[:, :, :63]\n",
    "    hand2_data = X[:, :, 63:]\n",
    "    return hand1_data, hand2_data\n",
    "\n",
    "def load_or_create_subject_datasets(X, y, subject_number, split_indices, data_folder):\n",
    "    X_original_path = os.path.join(data_folder, f'X_subject{subject_number}_original.npy')\n",
    "    y_original_path = os.path.join(data_folder, f'y_subject{subject_number}_original.npy')\n",
    "    X_aug_path = os.path.join(data_folder, f'X_subject{subject_number}_aug.npy')\n",
    "    y_aug_path = os.path.join(data_folder, f'y_subject{subject_number}_aug.npy')\n",
    "\n",
    "    files_exist = all(os.path.exists(p) for p in [X_original_path, y_original_path, X_aug_path, y_aug_path])\n",
    "\n",
    "    if files_exist:\n",
    "        print(f\"Loading datasets for Subject {subject_number} from disk.\")\n",
    "        X_subject_original = np.load(X_original_path)\n",
    "        y_subject_original = np.load(y_original_path)\n",
    "        X_subject_aug = np.load(X_aug_path)\n",
    "        y_subject_aug = np.load(y_aug_path)\n",
    "    else:\n",
    "        print(f\"Creating and saving datasets for Subject {subject_number}.\")\n",
    "        start_idx, end_idx = split_indices\n",
    "        X_subject_original = X[start_idx:end_idx]\n",
    "        y_subject_original = y[start_idx:end_idx]\n",
    "        np.save(X_original_path, X_subject_original)\n",
    "        np.save(y_original_path, y_subject_original)\n",
    "        X_subject_aug, y_subject_aug = augment_dataset(X_subject_original, y_subject_original, 10000)\n",
    "        np.save(X_aug_path, X_subject_aug)\n",
    "        np.save(y_aug_path, y_aug_path)\n",
    "\n",
    "    return (X_subject_original, y_subject_original), (X_subject_aug, y_subject_aug)\n",
    "\n",
    "input_shape = (30, 63)\n",
    "\n",
    "def prepare_datasets(train_subjects, test_subject):\n",
    "    X_train_full = np.vstack([subject_datasets[train][\"aug\"][0] for train in train_subjects])\n",
    "    y_train = np.vstack([subject_datasets[train][\"aug\"][1] for train in train_subjects])\n",
    "    X_test_full, y_test = subject_datasets[test_subject][\"original\"]\n",
    "    X_val_full, X_test_full, y_val, y_test = train_test_split(X_test_full, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    X_train_hand1, X_train_hand2 = split_hand_data(X_train_full)\n",
    "    X_val_hand1, X_val_hand2 = split_hand_data(X_val_full)\n",
    "    X_test_hand1, X_test_hand2 = split_hand_data(X_test_full)\n",
    "\n",
    "    return (X_train_hand1, X_train_hand2, y_train,\n",
    "            X_val_hand1, X_val_hand2, y_val,\n",
    "            X_test_hand1, X_test_hand2, y_test)\n",
    "\n",
    "def build_classification_model(bilstm_unit_size=128, mlp_units=[128,64], dropout=0.1):\n",
    "    left_input = tf.keras.Input(shape=input_shape, name=\"left_hand_input\")\n",
    "    right_input = tf.keras.Input(shape=input_shape, name=\"right_hand_input\")\n",
    "\n",
    "    # Left hand BiLSTM\n",
    "    left_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(bilstm_unit_size, return_sequences=True),\n",
    "        name=\"left_bilstm\"\n",
    "    )(left_input)  # shape: (30, 2*bilstm_unit_size)\n",
    "\n",
    "    # Project to (30,63)\n",
    "    left_projected = layers.Dense(63, activation='linear', name=\"left_project\")(left_bilstm_output)\n",
    "\n",
    "    # Right hand BiLSTM\n",
    "    right_bilstm_output = layers.Bidirectional(\n",
    "        layers.LSTM(bilstm_unit_size, return_sequences=True),\n",
    "        name=\"right_bilstm\"\n",
    "    )(right_input)  # shape: (30, 2*bilstm_unit_size)\n",
    "\n",
    "    # Project to (30,63)\n",
    "    right_projected = layers.Dense(63, activation='linear', name=\"right_project\")(right_bilstm_output)\n",
    "\n",
    "    # Concatenate: (30,63) + (30,63) = (30,126)\n",
    "    concatenated = layers.Concatenate(name=\"concat\")([left_projected, right_projected])\n",
    "\n",
    "    # Flatten (30,126) -> (3780,)\n",
    "    x = layers.Flatten(name=\"flatten\")(concatenated)\n",
    "\n",
    "    # MLP layers\n",
    "    for idx, units in enumerate(mlp_units):\n",
    "        x = layers.Dense(units, activation='gelu', name=f\"mlp_dense_{idx}\")(x)\n",
    "        x = layers.Dropout(dropout, name=f\"mlp_dropout_{idx}\")(x)\n",
    "\n",
    "    # Output layer for classification\n",
    "    output = layers.Dense(9, activation='softmax', name=\"classification_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[left_input, right_input], outputs=output, name=\"classification_model\")\n",
    "    return model\n",
    "\n",
    "X = np.load('NEW_DATASET/dataset_20240709_X_3096.npy')\n",
    "y = np.load('NEW_DATASET/dataset_20240709_y_3096.npy')\n",
    "\n",
    "data_folder = 'processed_datasets'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "split_60 = int(0.60 * len(X))\n",
    "split_80 = int(0.80 * len(X))\n",
    "\n",
    "(X_subject1, y_subject1), (X_subject1_aug, y_subject1_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=1, split_indices=(0, split_60), data_folder=data_folder)\n",
    "(X_subject2, y_subject2), (X_subject2_aug, y_subject2_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=2, split_indices=(split_60, split_80), data_folder=data_folder)\n",
    "(X_subject3, y_subject3), (X_subject3_aug, y_subject3_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=3, split_indices=(split_80, len(X)), data_folder=data_folder)\n",
    "\n",
    "subject_datasets = {\n",
    "    1: {\"aug\": (X_subject1_aug, y_subject1_aug), \"original\": (X_subject1, y_subject1)},\n",
    "    2: {\"aug\": (X_subject2_aug, y_subject2_aug), \"original\": (X_subject2, y_subject2)},\n",
    "    3: {\"aug\": (X_subject3_aug, y_subject3_aug), \"original\": (X_subject3, y_subject3)}\n",
    "}\n",
    "\n",
    "bilstm_unit_sizes = [32, 64, 128]  # You can pick any size now\n",
    "mlp_units = [128,64]\n",
    "dropouts = [0.1, 0.2]\n",
    "learning_rates = [2e-4, 5e-4, 8e-4]\n",
    "batch_size = 1024\n",
    "epochs = 200\n",
    "\n",
    "combinations_subjects = [\n",
    "    {'train_subjects': [1, 2], 'test_subject': 3},\n",
    "    {'train_subjects': [1, 3], 'test_subject': 2},\n",
    "    {'train_subjects': [2, 3], 'test_subject': 1}\n",
    "]\n",
    "\n",
    "for bilstm_unit_size in bilstm_unit_sizes:\n",
    "    for dropout in dropouts:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(\"\\n======================================\")\n",
    "            print(f\"Testing hyperparameters: bilstm_unit_size={bilstm_unit_size}, dropout={dropout}, lr={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "            print(\"======================================\\n\")\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "            f1_scores = []\n",
    "\n",
    "            for idx, combo in enumerate(combinations_subjects):\n",
    "                print(f\"\\nRunning combination {idx+1}: Train on subjects {combo['train_subjects']}, Test on subject {combo['test_subject']}\")\n",
    "\n",
    "                train_subjects = combo['train_subjects']\n",
    "                test_subject = combo['test_subject']\n",
    "\n",
    "                X_train_hand1, X_train_hand2, y_train, \\\n",
    "                X_val_hand1, X_val_hand2, y_val, \\\n",
    "                X_test_hand1, X_test_hand2, y_test = prepare_datasets(train_subjects, test_subject)\n",
    "\n",
    "                classification_model = build_classification_model(\n",
    "                    bilstm_unit_size=bilstm_unit_size,\n",
    "                    mlp_units=mlp_units,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0)\n",
    "                classification_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "                def lr_schedule(epoch):\n",
    "                    warmup_epochs = 20\n",
    "                    total_epochs = epochs\n",
    "                    initial_lr = learning_rate\n",
    "                    min_lr = 1e-8\n",
    "\n",
    "                    if epoch < warmup_epochs:\n",
    "                        lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "                    else:\n",
    "                        decay_epochs = total_epochs - warmup_epochs\n",
    "                        lr = initial_lr - (initial_lr - min_lr) * (epoch - warmup_epochs + 1) / decay_epochs\n",
    "                        lr = max(lr, min_lr)\n",
    "                    return lr\n",
    "\n",
    "                from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "                callbacks = [LearningRateScheduler(lr_schedule, verbose=0)]\n",
    "\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                    ((X_train_hand1, X_train_hand2), y_train)\n",
    "                ).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                    ((X_val_hand1, X_val_hand2), y_val)\n",
    "                ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                classification_model.fit(\n",
    "                    train_dataset,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                    ((X_test_hand1, X_test_hand2), y_test)\n",
    "                ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "                y_pred_probs = classification_model.predict(test_dataset)\n",
    "                y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "                y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "                f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "                test_loss, test_accuracy = classification_model.evaluate(test_dataset, verbose=0)\n",
    "                print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "            # Average metrics over three subject combinations\n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "            avg_test_accuracy = np.mean(test_accuracies)\n",
    "            avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "            print(\"\\n*** AVERAGED RESULTS FOR THIS HYPERPARAMETER SET ***\")\n",
    "            print(f'Average Test Loss: {avg_test_loss:.4f}')\n",
    "            print(f'Average Test Accuracy: {avg_test_accuracy:.4f}')\n",
    "            print(f'Average F1 Score: {avg_f1_score:.4f}')\n",
    "            print(\"****************************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e1ed5-094a-467f-84e0-b0a51979c4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
