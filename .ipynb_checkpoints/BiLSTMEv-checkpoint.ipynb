{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746d6be2-6964-4c12-97d7-7ad4d0a18161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "Loading datasets for Subject 1 from disk.\n",
      "Loading datasets for Subject 2 from disk.\n",
      "Loading datasets for Subject 3 from disk.\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 64, 'dropout_rate': 0.1, 'learning_rate': 0.01}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 5ms/step\n",
      "Train Acc: 0.9999, Val Acc: 0.8839, Test Acc: 0.8839, F1: 0.8885\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 0.9999, Val Acc: 0.6246, Test Acc: 0.6935, F1: 0.6879\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9999, Val Acc: 0.7252, Test Acc: 0.7083, F1: 0.7092\n",
      "Average Train Acc: 0.9999, Average Val Acc: 0.7446, Average Test Acc: 0.7619, Average F1: 0.7619\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 64, 'dropout_rate': 0.1, 'learning_rate': 0.0075250000000000004}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 5ms/step\n",
      "Train Acc: 0.9998, Val Acc: 0.8548, Test Acc: 0.8839, F1: 0.8903\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 0.9990, Val Acc: 0.6181, Test Acc: 0.6548, F1: 0.6403\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9996, Val Acc: 0.7004, Test Acc: 0.6878, F1: 0.6867\n",
      "Average Train Acc: 0.9995, Average Val Acc: 0.7245, Average Test Acc: 0.7422, Average F1: 0.7391\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 64, 'dropout_rate': 0.1, 'learning_rate': 0.00505}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 0.9996, Val Acc: 0.8677, Test Acc: 0.8935, F1: 0.8962\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9999, Val Acc: 0.5890, Test Acc: 0.6290, F1: 0.6150\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9998, Val Acc: 0.6606, Test Acc: 0.6555, F1: 0.6567\n",
      "Average Train Acc: 0.9998, Average Val Acc: 0.7058, Average Test Acc: 0.7260, Average F1: 0.7226\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 64, 'dropout_rate': 0.1, 'learning_rate': 0.002574999999999999}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 0.9996, Val Acc: 0.8387, Test Acc: 0.8484, F1: 0.8478\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9999, Val Acc: 0.5987, Test Acc: 0.6516, F1: 0.6361\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9997, Val Acc: 0.6659, Test Acc: 0.6566, F1: 0.6497\n",
      "Average Train Acc: 0.9997, Average Val Acc: 0.7011, Average Test Acc: 0.7189, Average F1: 0.7112\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 64, 'dropout_rate': 0.1, 'learning_rate': 0.0001}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 0.8857, Val Acc: 0.7161, Test Acc: 0.7129, F1: 0.7193\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9027, Val Acc: 0.5437, Test Acc: 0.6129, F1: 0.5968\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9135, Val Acc: 0.6056, Test Acc: 0.6211, F1: 0.6159\n",
      "Average Train Acc: 0.9006, Average Val Acc: 0.6218, Average Test Acc: 0.6490, Average F1: 0.6440\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 128, 'dropout_rate': 0.1, 'learning_rate': 0.01}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.8903, Test Acc: 0.8903, F1: 0.8946\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.6052, Test Acc: 0.6548, F1: 0.6371\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9999, Val Acc: 0.7037, Test Acc: 0.7018, F1: 0.6972\n",
      "Average Train Acc: 1.0000, Average Val Acc: 0.7331, Average Test Acc: 0.7490, Average F1: 0.7430\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 128, 'dropout_rate': 0.1, 'learning_rate': 0.0075250000000000004}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.8903, Test Acc: 0.8935, F1: 0.8972\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.5922, Test Acc: 0.6484, F1: 0.6262\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.6994, Test Acc: 0.6835, F1: 0.6839\n",
      "Average Train Acc: 1.0000, Average Val Acc: 0.7273, Average Test Acc: 0.7418, Average F1: 0.7358\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 128, 'dropout_rate': 0.1, 'learning_rate': 0.00505}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.8839, Test Acc: 0.8839, F1: 0.8871\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 7ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.5728, Test Acc: 0.6548, F1: 0.6419\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.7069, Test Acc: 0.6975, F1: 0.6977\n",
      "Average Train Acc: 1.0000, Average Val Acc: 0.7212, Average Test Acc: 0.7454, Average F1: 0.7422\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 128, 'dropout_rate': 0.1, 'learning_rate': 0.002574999999999999}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 6ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.8419, Test Acc: 0.8516, F1: 0.8559\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.6117, Test Acc: 0.6290, F1: 0.6206\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 1.0000, Val Acc: 0.6336, Test Acc: 0.6437, F1: 0.6362\n",
      "Average Train Acc: 1.0000, Average Val Acc: 0.6957, Average Test Acc: 0.7081, Average F1: 0.7043\n",
      "\n",
      "Evaluating Hyperparameters: {'lstm_units': 128, 'dropout_rate': 0.1, 'learning_rate': 0.0001}\n",
      "Training on Subjects [1, 2], Testing on Subject 3\n",
      "10/10 [==============================] - 1s 4ms/step\n",
      "Train Acc: 0.9542, Val Acc: 0.7290, Test Acc: 0.7452, F1: 0.7554\n",
      "Training on Subjects [1, 3], Testing on Subject 2\n",
      "10/10 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9523, Val Acc: 0.5922, Test Acc: 0.6323, F1: 0.6249\n",
      "Training on Subjects [2, 3], Testing on Subject 1\n",
      "30/30 [==============================] - 1s 3ms/step\n",
      "Train Acc: 0.9647, Val Acc: 0.5776, Test Acc: 0.5963, F1: 0.5879\n",
      "Average Train Acc: 0.9570, Average Val Acc: 0.6330, Average Test Acc: 0.6579, Average F1: 0.6561\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import CubicSpline\n",
    "from itertools import product\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed_value=42):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Enable mixed precision if desired\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "tf.config.optimizer.set_experimental_options({\n",
    "    'layout_optimizer': True,\n",
    "    'constant_folding': True,\n",
    "    'shape_optimization': True,\n",
    "    'remapping': True,\n",
    "    'arithmetic_optimization': True,\n",
    "    'dependency_optimization': True,\n",
    "    'loop_optimization': True,\n",
    "    'function_optimization': True,\n",
    "    'auto_mixed_precision': True,\n",
    "    'auto_parallel': True\n",
    "})\n",
    "\n",
    "# Load the dataset\n",
    "X = np.load('NEW_DATASET/dataset_20240709_X_3096.npy')\n",
    "y = np.load('NEW_DATASET/dataset_20240709_y_3096.npy')\n",
    "\n",
    "# Define the subject split points\n",
    "split_60 = int(0.60 * len(X))\n",
    "split_80 = int(0.80 * len(X))\n",
    "\n",
    "# Data augmentation functions remain the same\n",
    "def add_noise(data, noise_factor=0.0185):\n",
    "    noise = np.random.randn(*data.shape) * noise_factor\n",
    "    return np.clip(data + noise, 0, 1)\n",
    "\n",
    "def scale(data, scale_factor=0.1605):\n",
    "    scale = 1 + np.random.uniform(-scale_factor, scale_factor)\n",
    "    return np.clip(data * scale, 0, 1)\n",
    "\n",
    "def generate_random_curves(data, sigma=0.078, knot=2):\n",
    "    xx = np.linspace(0, data.shape[0] - 1, knot + 2)\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, data.shape[1]))\n",
    "    x_range = np.arange(data.shape[0])\n",
    "    augmented_data = np.zeros_like(data)\n",
    "    for i in range(data.shape[1]):\n",
    "        cs = CubicSpline(xx, yy[:, i])\n",
    "        augmented_data[:, i] = data[:, i] * cs(x_range)\n",
    "    return np.clip(augmented_data, 0, 1)\n",
    "\n",
    "def augment_sample(sample):\n",
    "    sample = add_noise(sample)\n",
    "    sample = scale(sample)\n",
    "    sample = generate_random_curves(sample)\n",
    "    return sample\n",
    "\n",
    "# Adjusted augment_dataset function remains the same\n",
    "def augment_dataset(X, y, target_size=10000):\n",
    "    num_original = len(X)\n",
    "    num_augmented = target_size - num_original\n",
    "\n",
    "    indices = np.random.randint(0, num_original, size=num_augmented)\n",
    "\n",
    "    def process_sample(idx):\n",
    "        augmented_x = augment_sample(X[idx])\n",
    "        return augmented_x, y[idx]\n",
    "\n",
    "    augmented_samples = Parallel(n_jobs=-1)(\n",
    "        delayed(process_sample)(idx) for idx in indices\n",
    "    )\n",
    "\n",
    "    augmented_X = np.array([sample[0] for sample in augmented_samples])\n",
    "    augmented_y = np.array([sample[1] for sample in augmented_samples])\n",
    "    return np.vstack((X, augmented_X)), np.vstack((y, augmented_y))\n",
    "\n",
    "# Create data folder if it doesn't exist\n",
    "data_folder = 'processed_datasets'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "# Split indices for the subjects\n",
    "split_60 = int(0.60 * len(X))\n",
    "split_80 = int(0.80 * len(X))\n",
    "\n",
    "# Function to load or create datasets for each subject remains the same\n",
    "def load_or_create_subject_datasets(X, y, subject_number, split_indices, data_folder):\n",
    "    # Determine file paths\n",
    "    X_original_path = os.path.join(data_folder, f'X_subject{subject_number}_original.npy')\n",
    "    y_original_path = os.path.join(data_folder, f'y_subject{subject_number}_original.npy')\n",
    "    X_aug_path = os.path.join(data_folder, f'X_subject{subject_number}_aug.npy')\n",
    "    y_aug_path = os.path.join(data_folder, f'y_subject{subject_number}_aug.npy')\n",
    "\n",
    "    # Check if files exist\n",
    "    files_exist = all(os.path.exists(p) for p in [X_original_path, y_original_path, X_aug_path, y_aug_path])\n",
    "\n",
    "    if files_exist:\n",
    "        print(f\"Loading datasets for Subject {subject_number} from disk.\")\n",
    "        # Load datasets\n",
    "        X_subject_original = np.load(X_original_path)\n",
    "        y_subject_original = np.load(y_original_path)\n",
    "        X_subject_aug = np.load(X_aug_path)\n",
    "        y_subject_aug = np.load(y_aug_path)\n",
    "    else:\n",
    "        print(f\"Creating and saving datasets for Subject {subject_number}.\")\n",
    "        # Split data for the subject\n",
    "        start_idx, end_idx = split_indices\n",
    "        X_subject_original = X[start_idx:end_idx]\n",
    "        y_subject_original = y[start_idx:end_idx]\n",
    "        # Save original data\n",
    "        np.save(X_original_path, X_subject_original)\n",
    "        np.save(y_original_path, y_subject_original)\n",
    "        # Augment data\n",
    "        X_subject_aug, y_subject_aug = augment_dataset(X_subject_original, y_subject_original, 10000)\n",
    "        # Save augmented data\n",
    "        np.save(X_aug_path, X_subject_aug)\n",
    "        np.save(y_aug_path, y_subject_aug)\n",
    "\n",
    "    return (X_subject_original, y_subject_original), (X_subject_aug, y_subject_aug)\n",
    "\n",
    "# Load or create datasets for subjects\n",
    "(X_subject1, y_subject1), (X_subject1_aug, y_subject1_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=1, split_indices=(0, split_60), data_folder=data_folder)\n",
    "\n",
    "(X_subject2, y_subject2), (X_subject2_aug, y_subject2_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=2, split_indices=(split_60, split_80), data_folder=data_folder)\n",
    "\n",
    "(X_subject3, y_subject3), (X_subject3_aug, y_subject3_aug) = load_or_create_subject_datasets(\n",
    "    X, y, subject_number=3, split_indices=(split_80, len(X)), data_folder=data_folder)\n",
    "\n",
    "# Store datasets in a dictionary remains the same\n",
    "subject_datasets = {\n",
    "    1: {\"aug\": (X_subject1_aug, y_subject1_aug), \"original\": (X_subject1, y_subject1)},\n",
    "    2: {\"aug\": (X_subject2_aug, y_subject2_aug), \"original\": (X_subject2, y_subject2)},\n",
    "    3: {\"aug\": (X_subject3_aug, y_subject3_aug), \"original\": (X_subject3, y_subject3)}\n",
    "}\n",
    "\n",
    "# Prepare datasets for given train/test subject combinations remains the same\n",
    "def prepare_datasets(train_subjects, test_subject):\n",
    "    X_train = np.vstack([subject_datasets[train][\"aug\"][0] for train in train_subjects])\n",
    "    y_train = np.vstack([subject_datasets[train][\"aug\"][1] for train in train_subjects])\n",
    "    X_test, y_test = subject_datasets[test_subject][\"original\"]\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Bi-LSTM model\n",
    "def build_bilstm_model(input_shape, lstm_units=32, dropout_rate=0.0):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False), input_shape=input_shape),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation='gelu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(9, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Custom learning rate scheduler with warmup and linear decay\n",
    "class WarmupDecayLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_lr, warmup_epochs, total_epochs, min_lr=1e-8):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            lr = (self.initial_lr / self.warmup_epochs) * (epoch + 1)\n",
    "        else:\n",
    "            decay_steps = self.total_epochs - self.warmup_epochs\n",
    "            lr = self.initial_lr * max((decay_steps - (epoch - self.warmup_epochs)) / decay_steps, self.min_lr)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparameters):\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    model = build_bilstm_model(input_shape=(30, 126), lstm_units=hyperparameters['lstm_units'], dropout_rate=hyperparameters['dropout_rate'])\n",
    "\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "    batch_size = 2048  # Adjustable batch size\n",
    "\n",
    "    # Create TensorFlow datasets with adjustable batch size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Learning rate scheduler and early stopping\n",
    "    lr_scheduler = WarmupDecayLearningRateScheduler(initial_lr=learning_rate, warmup_epochs=20, total_epochs=200)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=200, validation_data=val_dataset, \n",
    "        callbacks=[lr_scheduler, early_stopping], verbose=0\n",
    "    )\n",
    "\n",
    "    train_acc = history.history['categorical_accuracy'][-1]\n",
    "    val_acc = history.history['val_categorical_accuracy'][-1]\n",
    "    test_loss, test_acc = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    f1 = classification_report(y_true, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, F1: {f1:.4f}\")\n",
    "    return train_acc, val_acc, test_acc, f1\n",
    "\n",
    "def run_experiments():\n",
    "    lstm_units_list = [64, 128, 256]\n",
    "    dropout_rates = [0.1,0.2]\n",
    "    learning_rates = np.linspace(1e-2, 1e-4, 5)\n",
    "\n",
    "    for lstm_units, dropout_rate, learning_rate in product(lstm_units_list, dropout_rates, learning_rates):\n",
    "        hyperparameters = {'lstm_units': lstm_units, 'dropout_rate': dropout_rate, 'learning_rate': learning_rate}\n",
    "        print(f\"\\nEvaluating Hyperparameters: {hyperparameters}\")\n",
    "\n",
    "        results = []\n",
    "        for train_subjects, test_subject in [([1, 2], 3), ([1, 3], 2), ([2, 3], 1)]:\n",
    "            print(f\"Training on Subjects {train_subjects}, Testing on Subject {test_subject}\")\n",
    "            X_train, y_train, X_val, y_val, X_test, y_test = prepare_datasets(train_subjects, test_subject)\n",
    "            results.append(train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparameters))\n",
    "\n",
    "        avg_train_acc = np.mean([res[0] for res in results])\n",
    "        avg_val_acc = np.mean([res[1] for res in results])\n",
    "        avg_test_acc = np.mean([res[2] for res in results])\n",
    "        avg_f1 = np.mean([res[3] for res in results])\n",
    "\n",
    "        print(f\"Average Train Acc: {avg_train_acc:.4f}, Average Val Acc: {avg_val_acc:.4f}, Average Test Acc: {avg_test_acc:.4f}, Average F1: {avg_f1:.4f}\")\n",
    "\n",
    "run_experiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5a4ba-fcb1-478e-89b0-3b680364c296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
