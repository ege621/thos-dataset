{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7b0e6-9cca-4fb3-a09e-8ba4a0c7a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "\n",
    "# Rebuild THOSnet model structure\n",
    "def transformer_decoder_block(query, key_value, head_size, num_heads, ff_dim, dropout=0.3):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(query)\n",
    "    x1 = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x1 = layers.Dropout(dropout)(x1)\n",
    "    x = layers.Add()([x1, query])\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    y1 = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(y, key_value)\n",
    "    y1 = layers.Dropout(dropout)(y1)\n",
    "    y = layers.Add()([y1, x])\n",
    "    z = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    z1 = layers.Dense(ff_dim, activation='gelu')(z)\n",
    "    z1 = layers.Dropout(dropout)(z1)\n",
    "    z1 = layers.Dense(query.shape[-1])(z1)\n",
    "    return layers.Add()([z1, y])\n",
    "\n",
    "def build_thosnet(input_shape=(30,63)):\n",
    "    left_in  = tf.keras.Input(shape=input_shape, name=\"left_hand\")\n",
    "    right_in = tf.keras.Input(shape=input_shape, name=\"right_hand\")\n",
    "    L = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(left_in)\n",
    "    R = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(right_in)\n",
    "    decA = transformer_decoder_block(L, R, head_size=256, num_heads=8, ff_dim=64)\n",
    "    decB = transformer_decoder_block(R, L, head_size=256, num_heads=8, ff_dim=64)\n",
    "    merged = layers.Concatenate()([decA, decB])\n",
    "    flat   = layers.Flatten()(merged)\n",
    "    x = layers.Dense(128, activation='gelu')(flat)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='gelu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(9, activation='softmax')(x)\n",
    "    return tf.keras.Model([left_in, right_in], out)\n",
    "\n",
    "model = build_thosnet()\n",
    "\n",
    "#You can find the weights file from the repo under Google Drive.\n",
    "model.load_weights('models/thosnet_weights.h5')\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.75)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "buffer = deque(maxlen=30)\n",
    "class_names = [f\"Gesture_{i}\" for i in range(9)]\n",
    "cap = cv2.VideoCapture(0,cv2.CAP_AVFOUNDATION) #CHange 0 if you have multiple cameras. I added the second argument to try out GoPro's for predictions, you can remove it.\n",
    "\n",
    "def draw_label(img, text, pos, bg_color=(0,0,0), text_color=(0,255,0), font_scale=2, thickness=1):\n",
    "    \"\"\"Draws a semi-transparent box with text and a subtle shadow.\"\"\"\n",
    "    x, y = pos\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    # get text size\n",
    "    (w, h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "    padding = 10\n",
    "    # draw semi-transparent rectangle\n",
    "    overlay = img.copy()\n",
    "    cv2.rectangle(overlay, (x, y - h - padding//2), (x + w + padding, y + padding), bg_color, cv2.FILLED)\n",
    "    alpha = 0.6\n",
    "    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)\n",
    "    # draw drop-shadow\n",
    "    shadow_off = 2\n",
    "    cv2.putText(img, text, (x + shadow_off, y + shadow_off), font, font_scale, (0,0,0), thickness+1, cv2.LINE_AA)\n",
    "    # draw main text\n",
    "    cv2.putText(img, text, (x, y), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "#Currently, there is no \"No Gesture\" class, so the loop will always try to classify your hands if they are present within the frame.\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    features = np.zeros((2,21,3))\n",
    "    if results.multi_hand_landmarks:\n",
    "        for lm, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            coords = np.array([[p.x, p.y, p.z] for p in lm.landmark])\n",
    "            idx = 0 if handedness.classification[0].label=='Left' else 1\n",
    "            features[idx] = coords\n",
    "            mp_draw.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    lh_vec = features[0].reshape(-1)\n",
    "    rh_vec = features[1].reshape(-1)\n",
    "    buffer.append(np.concatenate([lh_vec, rh_vec]))\n",
    "\n",
    "    if len(buffer) == 30:\n",
    "        seq = np.array(buffer)[None,:,:]\n",
    "        p = model.predict([seq[:,:, :63], seq[:,:,63:]], verbose=0)\n",
    "        label = class_names[np.argmax(p)]\n",
    "    else:\n",
    "        label = \"Collecting...\"\n",
    "\n",
    "    # draw top-left label box\n",
    "    draw_label(frame, label, (15, 110), bg_color=(30,30,30), text_color=(50,230,50))\n",
    "\n",
    "    cv2.imshow('THOSnet Live', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969dca7-3dcf-4686-9d81-0f9dfda7f3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
